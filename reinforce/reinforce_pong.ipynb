{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pong_utils\n",
    "%matplotlib inline\n",
    "\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        # 80x80 to outputsize x outputsize\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride \n",
    "        # (round up if not an integer)\n",
    "\n",
    "        # conv1 : 80 x 80 -> 40 x 40\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=2, stride=2)\n",
    "        # conv2 : 40 x 40 -> 20 x 20\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=2, stride=2)\n",
    "        # conv3 : 20 x 20 -> 10 x 10\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=2, stride=2)\n",
    "        # conv4 : 10 x 10 ->  5 x  5\n",
    "        self.conv4 = nn.Conv2d(16, 32, kernel_size=2, stride=2)\n",
    "        self.size = 32 * 5 * 5\n",
    "        \n",
    "        # 1 fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 64)\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = x.view(-1, self.size)\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        x = F.relu(self.fc2(x))  \n",
    "        x = self.sig(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# use your own policy!\n",
    "# policy=Policy().to(device)\n",
    "policy=Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3190641f2c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "state = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(0)\n",
    "    state = next_state\n",
    "    print(reward)\n",
    "    time.sleep(0.1)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = pong_utils.parallelEnv('PongDeterministic-v4', n=4, seed=12345)\n",
    "prob, state, action, reward = pong_utils.collect_trajectories(envs, policy, tmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_future_rewards(rewards, ratio=0.999):\n",
    "    n = rewards.shape[1]\n",
    "    step = torch.arange(n)[:,None] - torch.arange(n)[None,:]\n",
    "    ones = torch.ones_like(step)\n",
    "    zeros = torch.zeros_like(step)\n",
    "    \n",
    "    target = torch.where(step >= 0, ones, zeros)\n",
    "    step = torch.where(step >= 0, step, zeros)    \n",
    "    discount = target * (ratio ** step)\n",
    "    discount = discount.to(device)\n",
    "    \n",
    "    rewards_discounted = torch.mm(rewards, discount)\n",
    "    return rewards_discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    \n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = pong_utils.states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    # discounted cumulative reward\n",
    "    R_future = discounted_future_rewards(rewards, discount)\n",
    "\n",
    "    # subtract baseline (= mean of reward)\n",
    "    R_mean = torch.mean(R_future)\n",
    "    R_future -= R_mean\n",
    "\n",
    "    # policy gradient maxmize target\n",
    "    surrogates = (R_future * torch.log(new_probs)).mean()\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # which prevents policy to become exactly 0 or 1\n",
    "    # this helps with exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    # entropy = -(new_probs*torch.log(old_probs+1.e-10) + (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "    # surrogates += torch.mean(beta*entropy)\n",
    "\n",
    "    return surrogates\n",
    "\n",
    "\n",
    "Lsur= surrogate(policy, prob, state, action, reward)\n",
    "\n",
    "print(Lsur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 2000\n",
    "# episode = 800\n",
    "\n",
    "# widget bar to display progress\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "# initialize environment\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "beta = .01\n",
    "tmax = 100\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    \n",
    "    L = -pong_utils.surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    del L\n",
    "        \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract bkg and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n",
    "                                    axis=-1)/255.\n",
    "    batch_input = np.swapaxes(list_of_images_prepro,0,1)\n",
    "    return batch_input\n",
    "\n",
    "g = preprocess_batch([frame1, frame2])\n",
    "h = torch.from_numpy(g).float().cuda()\n",
    "print(policy(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "state = env.reset()\n",
    "for _ in range(5):\n",
    "    frame1, reward1, done, _ = env.step(np.random.choice([4,5]))\n",
    "    frame2, reward2, done, _ = env.step(0)\n",
    "if not done:\n",
    "    for _ in range(2000):\n",
    "        env.render()\n",
    "        frame_input = pong_utils.preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "        action = 4 if random.random() < prob else 5\n",
    "        frame1, _, done, _ = env.step(action)\n",
    "        frame2, _, done, _ = env.step(0)\n",
    "        time.sleep(0.1)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'REINFORCE.policy')\n",
    "\n",
    "# load your policy if needed\n",
    "# policy = torch.load('REINFORCE.policy')\n",
    "\n",
    "# try and test out the solution!\n",
    "# policy = torch.load('PPO_solution.policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
