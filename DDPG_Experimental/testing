{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import Actor, Critic, hidden_init\n",
    "import gym\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "action_size = env.action_space.shape[0]\n",
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CRITIC = 0.001\n",
    "LR_ACTOR = 0.001\n",
    "TAU = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.actor_local = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, seed).to(device)\n",
    "        self.qvalue_local = Critic(state_size, action_size, seed).to(device)\n",
    "        self.qvalue_target = Critic(state_size, action_size, seed).to(device)\n",
    "        self.seed = seed\n",
    "\n",
    "\n",
    "        self.actor_target.load_state_dict(self.actor_local.state_dict())\n",
    "        self.qvalue_target.load_state_dict(self.qvalue_local.state_dict())\n",
    "        self.noise = OUNoise(action_size, self.seed)\n",
    "\n",
    "        self.qvalue_optimizer = optim.Adam(self.qvalue_local.parameters(), lr=LR_CRITIC)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "    def reset(self):\n",
    "        self.actor_local.reset_parameters()\n",
    "        self.qvalue_local.reset_parameters()\n",
    "        self.actor_target.reset_parameters()\n",
    "        self.qvalue_target.reset_parameters()\n",
    "        self.noise.reset()\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        # state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def into_tensor(self, state):\n",
    "        return torch.tensor(state).float().to(device)\n",
    "\n",
    "    def play2(self, max_len = 200):\n",
    "        state1 = env.reset()\n",
    "        state1 = self.into_tensor(state1)\n",
    "        score = 0\n",
    "        for _ in range(max_len):\n",
    "            action1 = self.act(state1)\n",
    "            action1_t = self.into_tensor(action1)\n",
    "            value1_pred = self.qvalue_local(state1, action1_t)\n",
    "\n",
    "            state2, reward, done, _ = env.step(action1)\n",
    "\n",
    "            state2 = self.into_tensor(state2)\n",
    "            action2 = self.actor_target(state2)\n",
    "            action2_t = self.into_tensor(action2)\n",
    "            value2 = self.qvalue_target(state2, action2_t)\n",
    "\n",
    "            done2 = True\n",
    "            if not done:\n",
    "                state3, _, done2, _ = env.step(action2)\n",
    "                state3 = self.into_tensor(state3)\n",
    "                action3 = slf.actor_target(state3)\n",
    "                action3_t = self.into_tensor(action3)\n",
    "                value3 = self.qvalue_target(state3, action3_t)\n",
    "            expected_value = self.into_tensor(reward) + GAMMA*((1-done)*value2 + (1-done2)*value3)\n",
    "            score += reward\n",
    "            ## Critic Update\n",
    "            qvalue_loss = F.mse_loss(value1_pred, expected_value)\n",
    "            self.qvalue_optimizer.zero_grad()\n",
    "            qvalue_loss.backward()\n",
    "            self.qvalue_optimizer.step()\n",
    "\n",
    "            ## Actor Update\n",
    "            actor_loss = -self.qvalue_target(state1, action1_t)\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            state1 = state2\n",
    "            if done: break\n",
    "\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        self.soft_update(self.qvalue_local, self.qvalue_target, TAU) \n",
    "            # print(actor_lo    ss, qvalue_loss)\n",
    "        return score\n",
    "\n",
    "    def play(self, max_len = 200):\n",
    "        state1 = env.reset()\n",
    "        state1 = self.into_tensor(state1)\n",
    "        score = 0\n",
    "        for _ in range(max_len):\n",
    "            # state1 = self.into_tensor(state1)\n",
    "            action1 = self.act(state1)\n",
    "            action1_t = self.into_tensor(action1)\n",
    "            value1_pred = self.qvalue_local(state1, action1_t)\n",
    "\n",
    "            state2, reward, done, _ = env.step(action1)\n",
    "\n",
    "            state2 = self.into_tensor(state2)\n",
    "            action2 = self.actor_target(state2)\n",
    "            action2_t = self.into_tensor(action2)\n",
    "            value2 = self.qvalue_target(state2, action2_t)\n",
    "\n",
    "            if not done:\n",
    "                expected_value = self.into_tensor(reward) + GAMMA * value2\n",
    "            else:\n",
    "                expected_value = self.into_tensor(reward)\n",
    "            score += reward\n",
    "            ## Critic Update\n",
    "            qvalue_loss = F.mse_loss(value1_pred, expected_value)\n",
    "            self.qvalue_optimizer.zero_grad()\n",
    "            qvalue_loss.backward()\n",
    "            self.qvalue_optimizer.step()\n",
    "\n",
    "            ## Actor Update\n",
    "            actor_loss = -self.qvalue_target(state1, action1_t)\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            state1 = state2  #updating state\n",
    "            if done: break\n",
    "            # print(actor_loss, qvalue_loss)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        self.soft_update(self.qvalue_local, self.qvalue_target, TAU) \n",
    "        return score\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def run(self, iteration, max_len = 200):\n",
    "        self.score_deque = deque(maxlen=100)\n",
    "        for episode in range(iteration):\n",
    "            score = self.play(max_len)\n",
    "            self.score_deque.append(score)\n",
    "            print(\"\\rIteration {} with Current Score {}\".format(episode, score), end=\" \")\n",
    "            if(episode%50 == 0):\n",
    "                print(\"\\rIteration {} with Average Score {} \".format(episode, sum(self.score_deque)/len(self.score_deque)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iteration 0 with Average Score -1103.3148470118442 \nIteration 50 with Average Score -1330.5977908575173 \nIteration 100 with Average Score -1294.5450420540005 \nIteration 150 with Average Score -1304.2150849042293 \nIteration 200 with Average Score -1327.147266309069 \nIteration 250 with Average Score -1319.0711027206198 \nIteration 300 with Average Score -1302.6068422255996 \nIteration 350 with Average Score -1253.2015933548228 \nIteration 400 with Average Score -1256.771512347519 \nIteration 450 with Average Score -1305.0721059689195 \nIteration 500 with Average Score -1321.612726022324 \nIteration 550 with Average Score -1299.399221409362 \nIteration 600 with Average Score -1309.7457630628835 \nIteration 650 with Average Score -1327.4931441382914 \nIteration 700 with Average Score -1330.8285106571775 \nIteration 750 with Average Score -1299.5565353441773 \nIteration 800 with Average Score -1278.0879658778265 \nIteration 850 with Average Score -1298.2096567180997 \nIteration 900 with Average Score -1308.1308075498816 \nIteration 950 with Average Score -1303.8722857349983 \nIteration 1000 with Average Score -1324.383635715279 \nIteration 1050 with Average Score -1305.418877097771 \nIteration 1100 with Average Score -1282.3136695469414 \nIteration 1150 with Average Score -1317.3689391706703 \nIteration 1200 with Average Score -1322.6409202020254 \nIteration 1250 with Average Score -1329.6040052188403 \nIteration 1300 with Average Score -1338.717351610222 \nIteration 1350 with Average Score -1339.0888602337589 \nIteration 1400 with Average Score -1324.870452351298 \nIteration 1450 with Average Score -1303.2241418667495 \nIteration 1500 with Average Score -1333.0973074171227 \nIteration 1550 with Average Score -1344.6441702824516 \nIteration 1600 with Average Score -1288.6970321821575 \nIteration 1650 with Average Score -1278.3472112394404 \nIteration 1700 with Average Score -1313.6883388909455 \nIteration 1750 with Average Score -1302.0015146854685 \nIteration 1800 with Average Score -1311.63634020737 \nIteration 1849 with Current Score -1202.9219434717916"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-23579723e1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-efa16d45605d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, iteration, max_len)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_deque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_deque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\rIteration {} with Current Score {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-efa16d45605d>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, max_len)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mqvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue1_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqvalue_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mqvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqvalue_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.run(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-0.16357517843353558"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "agent.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "while True:\n",
    "    tensor_state = torch.tensor(state).float().to(device)\n",
    "    action = actor_local(tensor_state).cpu().detach().numpy() + qnoise.sample()\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = torch.tensor(state).float().to(device)\n",
    "new_action = actor_local(tensor_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.0374], device='cuda:0', grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "qvalue_local.forward(new_state, new_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.0548, -0.0015,  0.0348], device='cuda:0', grad_fn=<CatBackward>)"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}