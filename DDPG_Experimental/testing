{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import Actor, Critic, hidden_init\n",
    "import gym\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "action_size = env.action_space.shape[0]\n",
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CRITIC = 0.001\n",
    "LR_ACTOR = 0.001\n",
    "TAU = 1e-3\n",
    "GAMMA = 0.99\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.actor_local = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, seed).to(device)\n",
    "        self.qvalue_local = Critic(state_size, action_size, seed).to(device)\n",
    "        self.qvalue_target = Critic(state_size, action_size, seed).to(device)\n",
    "        self.seed = seed\n",
    "\n",
    "\n",
    "        self.actor_target.load_state_dict(self.actor_local.state_dict())\n",
    "        self.qvalue_target.load_state_dict(self.qvalue_local.state_dict())\n",
    "        self.noise = OUNoise(action_size, self.seed)\n",
    "\n",
    "        self.qvalue_optimizer = optim.Adam(self.qvalue_local.parameters(), lr=LR_CRITIC)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "    def reset(self):\n",
    "        self.actor_local.reset_parameters()\n",
    "        self.qvalue_local.reset_parameters()\n",
    "        self.actor_target.reset_parameters()\n",
    "        self.qvalue_target.reset_parameters()\n",
    "        self.noise.reset()\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        # state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def into_tensor(self, state):\n",
    "        return torch.tensor(state).float().to(device)\n",
    "\n",
    "    def play2(self, max_len = 200):\n",
    "        state1 = env.reset()\n",
    "        state1 = self.into_tensor(state1)\n",
    "        score = 0\n",
    "        for _ in range(max_len):\n",
    "            action1 = self.act(state1)\n",
    "            action1_t = self.into_tensor(action1)\n",
    "            value1_pred = self.qvalue_local(state1, action1_t)\n",
    "\n",
    "            state2, reward, done, _ = env.step(action1)\n",
    "\n",
    "            state2 = self.into_tensor(state2)\n",
    "            action2 = self.actor_target(state2)\n",
    "            action2_t = self.into_tensor(action2)\n",
    "            value2 = self.qvalue_target(state2, action2_t)\n",
    "\n",
    "            done2 = True\n",
    "            if not done:\n",
    "                state3, _, done2, _ = env.step(action2)\n",
    "                state3 = self.into_tensor(state3)\n",
    "                action3 = slf.actor_target(state3)\n",
    "                action3_t = self.into_tensor(action3)\n",
    "                value3 = self.qvalue_target(state3, action3_t)\n",
    "            expected_value = self.into_tensor(reward) + GAMMA*((1-done)*value2 + (1-done2)*value3)\n",
    "            score += reward\n",
    "            ## Critic Update\n",
    "            qvalue_loss = F.mse_loss(value1_pred, expected_value)\n",
    "            self.qvalue_optimizer.zero_grad()\n",
    "            qvalue_loss.backward()\n",
    "            self.qvalue_optimizer.step()\n",
    "\n",
    "            ## Actor Update\n",
    "            actor_loss = -self.qvalue_target(state1, action1_t)\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            state1 = state2\n",
    "            if done: break\n",
    "\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        self.soft_update(self.qvalue_local, self.qvalue_target, TAU) \n",
    "            # print(actor_loss, qvalue_loss)\n",
    "        return score\n",
    "\n",
    "    def play(self, max_len = 200):\n",
    "        state1 = env.reset()\n",
    "        state1 = self.into_tensor(state1)\n",
    "        score = 0\n",
    "        for _ in range(max_len):\n",
    "            # state1 = self.into_tensor(state1)\n",
    "            action1 = self.act(state1)\n",
    "            action1_t = self.into_tensor(action1)\n",
    "            value1_pred = self.qvalue_local(state1, action1_t)\n",
    "\n",
    "            state2, reward, done, _ = env.step(action1)\n",
    "\n",
    "            state2 = self.into_tensor(state2)\n",
    "            action2 = self.actor_target(state2)\n",
    "            action2_t = self.into_tensor(action2)\n",
    "            value2 = self.qvalue_target(state2, action2_t)\n",
    "\n",
    "            if not done:\n",
    "                expected_value = self.into_tensor(reward) + GAMMA * value2\n",
    "            else:\n",
    "                expected_value = self.into_tensor(reward)\n",
    "            score += reward\n",
    "            ## Critic Update\n",
    "            qvalue_loss = F.mse_loss(value1_pred, expected_value)\n",
    "            self.qvalue_optimizer.zero_grad()\n",
    "            qvalue_loss.backward()\n",
    "            self.qvalue_optimizer.step()\n",
    "\n",
    "            ## Actor Update\n",
    "            actor_loss = -self.qvalue_target(state1, action1_t)\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            state1 = state2  #updating state\n",
    "            if done: break\n",
    "            # print(actor_loss, qvalue_loss)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        self.soft_update(self.qvalue_local, self.qvalue_target, TAU) \n",
    "        return score\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def run(self, iteration, max_len = 200):\n",
    "        self.score_deque = deque(maxlen=100)\n",
    "        for episode in range(iteration):\n",
    "            score = self.play(max_len)\n",
    "            self.score_deque.append(score)\n",
    "            print(\"\\rIteration {} with Current Score {}\".format(episode, score), end=\" \")\n",
    "            if(episode%50 == 0):\n",
    "                print(\"\\rIteration {} with Average Score {} \".format(episode, sum(self.score_deque)/len(self.score_deque)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iteration 0 with Average Score -1657.236645878832 \nIteration 50 with Average Score -1137.3262806172934 \nIteration 100 with Average Score -1150.6982334182057 \nIteration 150 with Average Score -1200.28958405762 \nIteration 200 with Average Score -1290.586763110965 \nIteration 250 with Average Score -1294.386501090354 \nIteration 300 with Average Score -1235.767023734978 \nIteration 330 with Current Score -1284.0588332334053"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-23579723e1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-39f523778067>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, iteration, max_len)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_deque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_deque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\rIteration {} with Current Score {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-39f523778067>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, max_len)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mstate2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mstate2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0maction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0maction2_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-39f523778067>\u001b[0m in \u001b[0;36minto_tensor\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.run(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-0.16357517843353558"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "agent.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "while True:\n",
    "    tensor_state = torch.tensor(state).float().to(device)\n",
    "    action = actor_local(tensor_state).cpu().detach().numpy() + qnoise.sample()\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = torch.tensor(state).float().to(device)\n",
    "new_action = actor_local(tensor_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.0374], device='cuda:0', grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "qvalue_local.forward(new_state, new_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.0548, -0.0015,  0.0348], device='cuda:0', grad_fn=<CatBackward>)"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}