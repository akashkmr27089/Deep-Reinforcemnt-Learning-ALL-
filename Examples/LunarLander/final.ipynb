{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import time \n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\ml-agents\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):  \n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=32):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 6)\n",
    "        self.fc3 = nn.Linear(6, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_dist(experiences):\n",
    "    states, actions, next_states, rewards = zip(*experiences)\n",
    "    next_states = torch.from_numpy(np.array(list(next_states)))\n",
    "    states = torch.from_numpy(np.array(list(states)))\n",
    "    rewards = torch.from_numpy(np.array(list(rewards)))\n",
    "    actions = torch.from_numpy(np.array(list(actions)))\n",
    "    return (states, actions, next_states, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exp_relay:\n",
    "    \n",
    "    def __init__(self, window = 1000, sample_size = 100):  \n",
    "        self.window = window\n",
    "        self.sample_size = sample_size\n",
    "        self.memory = []\n",
    "        \n",
    "    def size(self):\n",
    "        if len(self.memory) == self.window:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def add(self,experience):\n",
    "        if self.size() is True:\n",
    "            self.memory.append(experience)\n",
    "            del self.memory[0]\n",
    "        else:\n",
    "            self.memory.append(experience)\n",
    "    \n",
    "    def sample_mem(self):\n",
    "        return random.sample(self.memory, self.sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, gamma, alpha, eps_min=0.01):\n",
    "        self.epsilon = 1\n",
    "        self.eps_min = eps_min\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.eps_decay = 0.999\n",
    "        self.nA = env.action_space.n\n",
    "    \n",
    "    def q_prob(self, state, network):\n",
    "        policy = np.ones(self.nA)*(self.epsilon/self.nA)\n",
    "        qstate = network.forward(state)\n",
    "        best_pos = np.argmax(policy)\n",
    "        policy[best_pos] = (1 - self.epsilon) + (self.epsilon/self.nA)\n",
    "        return policy\n",
    "    \n",
    "    def action(self, state, policy_net):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.nA), p=self.q_prob(state, policy_net))\n",
    "        \n",
    "    def update(self, experiences, policy_net, target_net, optimizer):\n",
    "        states, actions, next_states, rewards = exp_dist(experiences)\n",
    "        data = policy_net.forward(states.to(device))\n",
    "        data_next = target_net.forward(next_states.to(device))\n",
    "        h = data_next.argmax(dim=1)\n",
    "        data_next = torch.tensor([data_next[(x,h[x])] for x in range(len(h))])\n",
    "        data = torch.tensor([data[(x,actions[x])] for x in range(len(actions))])\n",
    "        target_data = rewards + self.gamma*data_next\n",
    "        loss = F.mse_loss(target_data, data)\n",
    "        loss.requires_grad = True\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple('experience',['state','action','next_state','reward'])\n",
    "state_n = 8\n",
    "learning_rate = 0.1\n",
    "action_n = env.action_space.n\n",
    "policy_net = QNetwork(state_n, action_n).to(device)\n",
    "target_net = QNetwork(state_n, action_n).to(device)\n",
    "target_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1; window = 10000; sample = 1000; eps_min=0.2; alpha = 0.1\n",
    "update_freq = 100\n",
    "agent = Agent(gamma,alpha, eps_min)\n",
    "memory = exp_relay(window,sample)\n",
    "num_episode = 10000\n",
    "reward_data = []\n",
    "max_t = 200\n",
    "tau = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards nan with epsilon 0.999\n",
      "Average rewards nan with epsilon 0.998001\n",
      "Average rewards nan with epsilon 0.997002999\n",
      "Average rewards nan with epsilon 0.996005996001\n",
      "Average rewards nan with epsilon 0.995009990004999\n",
      "Average rewards nan with epsilon 0.994014980014994\n",
      "Average rewards nan with epsilon 0.993020965034979\n",
      "Average rewards nan with epsilon 0.9920279440699441\n",
      "Average rewards nan with epsilon 0.9910359161258742\n",
      "Average rewards nan with epsilon 0.9900448802097482\n",
      "Average rewards -142.63380178904902 with epsilon 0.9890548353295385\n",
      "Average rewards -200.95175997508403 with epsilon 0.988065780494209\n",
      "Average rewards -270.0626156664081 with epsilon 0.9870777147137147\n",
      "Average rewards -312.23973210478744 with epsilon 0.986090636999001\n",
      "Average rewards -282.595131684883 with epsilon 0.9851045463620021\n",
      "Average rewards -295.4639161820762 with epsilon 0.98411944181564\n",
      "Average rewards -288.71641009103 with epsilon 0.9831353223738244\n",
      "Average rewards -367.856287153833 with epsilon 0.9821521870514506\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,num_episode+1):\n",
    "    state = env.reset()\n",
    "    reward_val = 0\n",
    "    for target_param, local_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    for _ in range(max_t):\n",
    "        state = torch.from_numpy(state).to(device)\n",
    "        action = agent.action(state, policy_net)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.add(experience(state.cpu().numpy(),action,next_state,reward))\n",
    "#         agent.epsilon = max(agent.epsilon*agent.eps_decay, agent.eps_min)\n",
    "        reward_val += reward\n",
    "        if len(memory.memory) >= sample:\n",
    "            experiences = memory.sample_mem()\n",
    "            agent.update(experiences, policy_net,target_net,optimizer)\n",
    "        reward_val += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    reward_data.append(reward_val)\n",
    "    if i%1 == 0:\n",
    "        agent.epsilon = max(agent.epsilon*agent.eps_decay, agent.eps_min)\n",
    "        print(\"Average rewards {} with epsilon {}\".format(np.mean(reward_data[:-10]), agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences = []\n",
    "for _ in range(1):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = policy_net.forward(torch.from_numpy(state)).argmax().numpy()\n",
    "        #action = env.action_space.sample()agent.epsilon = max(agent.epsilon*agent.eps_decay, agent.eps_min)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        experiences.append(experience(state, action,reward,next_state))\n",
    "        state = next_state\n",
    "        time.sleep(0.1)\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, rewards = exp_dist(experiences)\n",
    "next_states = torch.from_numpy(np.array(list(next_states)))\n",
    "states = torch.from_numpy(np.array(list(states)))\n",
    "rewards = torch.from_numpy(np.array(list(rewards)))\n",
    "actions = torch.from_numpy(np.array(list(actions)))\n",
    "g = policy_net.forward(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [x for x in policy_net.parameters()]\n",
    "policy_net.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_param, local_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
