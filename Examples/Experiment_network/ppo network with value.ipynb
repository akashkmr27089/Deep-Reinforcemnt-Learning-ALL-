{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4,)\n",
      "Number of actions:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (final): Sigmoid()\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Critic(nn.Module):  #gives score of how bad or good the action is \n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "#         x = self.fc1(state)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = torch.tanh(x)   #using tanh for giving score of how good is action \n",
    "#         return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)   #using tanh for giving score of how good is action \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Actor(nn.Module):     #Policy Network\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32,action_size)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.final(x)    #using sigmoid in an action \n",
    "        return x    \n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "actor = Actor(4,1,12).to(device)\n",
    "critic = Critic(4,1,12).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=1e-4)\n",
    "print(actor)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 1.0 with action 0 with score tensor([0.], device='cuda:0', grad_fn=<ReluBackward0>) "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Making of Network using ppo Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards, next_states,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01,\n",
    "                     gamma = 0.1):\n",
    "\n",
    "    states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "    next_states = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards_te = np.multiply(rewards, discount).reshape(len(rewards),1)\n",
    "    rewards_future = rewards_te[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    ## adding contribution of actor\n",
    "    baseline_advantages = rewards_future + gamma*critic.forward(next_states).detach().cpu().numpy()\\\n",
    "                        - critic.forward(states).detach().cpu().numpy()\n",
    "    rewards_future = baseline_advantages.copy()\n",
    "    ##end\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis = 0)\n",
    "    std = np.std(rewards_future, axis = 0)\n",
    "    rewards_normalized = (rewards_future - mean)/std\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device).reshape(len(actions),1)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device).reshape(len(old_probs),1)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    \n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = policy.forward(states).reshape(states.size()[0],1)\n",
    "    new_probs = torch.where(actions == 1, new_probs, 1.0-new_probs)\n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "#     # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_baseline(next_state, reward, state):\n",
    "    next_state = torch.from_numpy(np.array(next_state)).to(device).float()\n",
    "    reward = torch.from_numpy(np.array(reward)).to(device)\n",
    "    state = torch.from_numpy(np.array(state)).to(device).float()\n",
    "    Loss = F.mse_loss(critic.forward(state), reward + critic.forward(next_state))\n",
    "    optimizer_critic.zero_grad()\n",
    "    Loss.backward()\n",
    "    optimizer_critic.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200):\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "    next_states = []\n",
    "    \n",
    "    for _ in range(tmax):\n",
    "        prob = actor(torch.from_numpy(state).float().to(device))   #for converting state to torch variable \n",
    "        probs.append(prob)\n",
    "        states.append(state)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done , _ = env.step(action)\n",
    "        update_baseline(next_state, reward,state)\n",
    "        next_states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return probs, states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 200\n",
    "SGD_epoch = 4\n",
    "episode = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                        | ETA:  0:00:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: 11.000000\n",
      "11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                      | ETA:  0:00:27\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: 8.000000\n",
      "8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  0:00:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, score: 10.000000\n",
      "10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  27% |###########                                | ETA:  0:00:22\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, score: 10.000000\n",
      "10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  33% |##############                             | ETA:  0:00:20\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, score: 8.000000\n",
      "8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  0:00:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120, score: 10.000000\n",
      "10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  47% |####################                       | ETA:  0:00:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 140, score: 10.000000\n",
      "10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  54% |#######################                    | ETA:  0:00:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 160, score: 9.000000\n",
      "9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |#########################                  | ETA:  0:00:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 180, score: 11.000000\n",
      "11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  63% |###########################                | ETA:  0:00:11\r"
     ]
    }
   ],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "#following generate sim_nos instance of simulation \n",
    "envs = gym.make('CartPole-v1')\n",
    "mean_rewards = []\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards, next_states = \\\n",
    "    collect_trajectories(envs, actor, tmax=tmax)  \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "    \n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    for _ in range(SGD_epoch):\n",
    "        L = -1*clipped_surrogate(actor, old_probs, states, actions, rewards, next_states, epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "\n",
    "    epsilon*=0.999\n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "    if(np.mean(total_rewards) == 200):\n",
    "        break\n",
    "    \n",
    "timer.finish()\n",
    "plt.plot(mean_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 1.0 with action 1 with score tensor([-0.9550], device='cuda:0', grad_fn=<TanhBackward>) "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_probs, states, actions, rewards, next_states = \\\n",
    "    collect_trajectories(envs, actor, tmax=tmax)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "torch_rewards = torch.from_numpy(np.array(rewards)).float().to(device).reshape(len(rewards),1)\n",
    "torch_next_states = torch.from_numpy(np.array(next_states)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g  = critic(torch_next_states).detach().cpu().numpy()\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state = torch.from_numpy(np.array(next_state)).to(device).float()\n",
    "reward = torch.from_numpy(np.array(reward)).to(device)\n",
    "state = torch.from_numpy(np.array(state)).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.forward(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_critic.zero_grad()\n",
    "h1.backward()\n",
    "optimizer_critic.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(state).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
