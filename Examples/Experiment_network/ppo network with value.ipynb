{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4,)\n",
      "Number of actions:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (final): Sigmoid()\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Critic(nn.Module):  #gives score of how bad or good the action is \n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.tanh(x)   #using tanh for giving score of how good is action \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Actor(nn.Module):     #Policy Network\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32,action_size)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.final(x)    #using sigmoid in an action \n",
    "        return x    \n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "actor = Actor(4,1,12).to(device)\n",
    "critic = Critic(4,1,12).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "print(actor)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 1.0 with action 0 with score tensor([-0.1284], device='cuda:0', grad_fn=<TanhBackward>) "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Making of Network using ppo Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "#     discount = discount**np.arange(len(rewards))\n",
    "#     rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "#     # convert rewards to future rewards\n",
    "#     rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "#     mean = np.mean(rewards_future, axis=1)\n",
    "#     std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "#     rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards_te = np.multiply(rewards, discount).reshape(len(rewards),1)\n",
    "    rewards_future = rewards_te[::-1].cumsum(axis=0)[::-1]\n",
    "    mean = np.mean(rewards_future, axis = 0)\n",
    "    std = np.std(rewards_future, axis = 0)\n",
    "    rewards_normalized = (rewards_future - mean)/std\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device).reshape(len(actions),1)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device).reshape(len(old_probs),1)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = policy.forward(states).reshape(states.size()[0],1)\n",
    "    new_probs = torch.where(actions == 1, new_probs, 1.0-new_probs)\n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "#     # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200):\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "    \n",
    "    for _ in range(tmax):\n",
    "        prob = actor(torch.from_numpy(state).float().to(device))   #for converting state to torch variable \n",
    "        probs.append(prob)\n",
    "        states.append(state)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done , _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return probs, states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 200\n",
    "SGD_epoch = 4\n",
    "episode = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  33% |##############                             | ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14110b40160>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "#following generate sim_nos instance of simulation \n",
    "envs = gym.make('CartPole-v1')\n",
    "mean_rewards = []\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "    collect_trajectories(envs, actor, tmax=tmax)  \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    for _ in range(SGD_epoch):\n",
    "        L = -1*clipped_surrogate(actor, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "\n",
    "    epsilon*=0.999\n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "    if(np.mean(total_rewards) == 200):\n",
    "        break\n",
    "    \n",
    "timer.finish()\n",
    "plt.plot(mean_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 1.0 with action 1 with score tensor([-0.0691], device='cuda:0', grad_fn=<TanhBackward>)                "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-68fec5183bec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./weights_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
