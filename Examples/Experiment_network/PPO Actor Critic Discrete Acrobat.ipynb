{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (6,)\n",
      "Number of actions:  3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x00000215A210BAE8>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\programdata\\anaconda3\\envs\\ml-agents\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 152, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\programdata\\anaconda3\\envs\\ml-agents\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 71, in close\n",
      "    self.window.close()\n",
      "  File \"c:\\programdata\\anaconda3\\envs\\ml-agents\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 299, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"c:\\programdata\\anaconda3\\envs\\ml-agents\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 823, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"c:\\programdata\\anaconda3\\envs\\ml-agents\\lib\\_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: (<weakref at 0x00000215F8CF79A8; to 'Win32Window' at 0x00000215FFEF8A90>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (final): Sigmoid()\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Critic(nn.Module):  #gives score of how bad or good the action is \n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "#         x = self.fc1(state)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = torch.tanh(x)   #using tanh for giving score of how good is action \n",
    "#         return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)   #using tanh for giving score of how good is action \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Actor(nn.Module):     #Policy Network\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32,action_size)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.final(x)    #using sigmoid in an action \n",
    "        return x    \n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "actor = Actor(6,3,12).to(device)\n",
    "critic = Critic(6,3,12).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=1e-4)\n",
    "print(actor)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with score tensor([0.1019, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>)           "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    for i in range(100):\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action = prob.argmax()\n",
    "        prob = max(prob)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Making of Network using ppo Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards, next_states,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01,\n",
    "                     gamma = 0.1):\n",
    "\n",
    "    states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "    next_states = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards_te = np.multiply(rewards, discount).reshape(len(rewards),1)\n",
    "    rewards_future = rewards_te[::-1].cumsum(axis=0)[::-1]\n",
    "    actions = np.array(actions, dtype=np.int8)\n",
    "    actions_final = torch.LongTensor(actions.reshape(len(actions),1))\n",
    "    \n",
    "#     # adding contribution of actor\n",
    "#     f1 = critic.forward(next_states).argmax(1).reshape(len(next_states),1)\n",
    "#     f2 = torch.LongTensor(f1.cpu().reshape(f1.size()[0],1))\n",
    "#     f3 = torch.gather(f1,1,f2.to(device))\n",
    "    \n",
    "# #     f1 = critic.forward(states).argmax(1).reshape(len(next_states),1)\n",
    "# #     f2 = torch.LongTensor(f1.cpu().reshape(f1.size()[0],1))\n",
    "# #     f4 = torch.gather(f1,1,f2.to(device))\n",
    "#     f1 = critic.forward(states)\n",
    "#     f4 = torch.gather(f1,1,actions_final.to(device))\n",
    "    \n",
    "#     rewards_future = rewards_future + gamma*f3.detach().cpu().numpy() - f4.detach().cpu().numpy()\n",
    "#     ##end\n",
    "    mean = np.mean(rewards_future, axis = 0)\n",
    "    std = np.std(rewards_future, axis = 0)\n",
    "    rewards_normalized = (rewards_future - mean)/std\n",
    "    \n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device).reshape(len(old_probs),1)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    \n",
    "    g = actor.forward(states)\n",
    "    new_probs = torch.gather(g,1,actions_final.to(device))\n",
    "    \n",
    "    ratio = new_probs/old_probs\n",
    "#     # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "    \n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_baseline(next_state, reward, state):\n",
    "    next_state = torch.from_numpy(np.array(next_state)).to(device).float()\n",
    "    reward = torch.from_numpy(np.array(reward)).to(device)\n",
    "    state = torch.from_numpy(np.array(state)).to(device).float()\n",
    "    Loss = F.mse_loss(critic.forward(state), reward + critic.forward(next_state))\n",
    "    optimizer_critic.zero_grad()\n",
    "    Loss.backward()\n",
    "    optimizer_critic.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200):\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "    next_states = []\n",
    "    \n",
    "    for _ in range(tmax):\n",
    "        prob = actor(torch.from_numpy(state).float().to(device))   #for converting state to torch variable \n",
    "        prob_new = max(prob)\n",
    "        probs.append(prob_new)\n",
    "        states.append(state)\n",
    "        action = prob.argmax()\n",
    "        next_state, reward, done , _ = env.step(action)\n",
    "#         update_baseline(next_state, reward,state)\n",
    "        next_states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return probs, states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, states, actions, rewards, next_states = collect_trajectories(env, actor, tmax=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 200\n",
    "SGD_epoch = 4\n",
    "episode = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                           | ETA:  0:06:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                          | ETA:  0:06:36\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                         | ETA:  0:06:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                        | ETA:  0:06:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |####                                       | ETA:  0:06:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                      | ETA:  0:06:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                      | ETA:  0:06:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 140, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  15% |######                                     | ETA:  0:05:54\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 160, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                    | ETA:  0:05:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 180, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  0:05:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  21% |#########                                  | ETA:  0:05:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 220, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  23% |##########                                 | ETA:  0:05:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 240, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |###########                                | ETA:  0:05:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 260, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  27% |###########                                | ETA:  0:05:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  29% |############                               | ETA:  0:04:52\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                              | ETA:  0:04:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 320, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  34% |##############                             | ETA:  0:04:35\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 340, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  35% |###############                            | ETA:  0:04:27\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 360, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  37% |################                           | ETA:  0:04:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 380, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  0:04:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  41% |#################                          | ETA:  0:04:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 420, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  43% |##################                         | ETA:  0:03:52\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 440, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  46% |###################                        | ETA:  0:03:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 460, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  48% |####################                       | ETA:  0:03:36\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 480, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  49% |#####################                      | ETA:  0:03:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  51% |######################                     | ETA:  0:03:20\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 520, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  53% |#######################                    | ETA:  0:03:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 540, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  55% |########################                   | ETA:  0:03:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 560, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  58% |########################                   | ETA:  0:02:54\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 580, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  59% |#########################                  | ETA:  0:02:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 600, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  61% |##########################                 | ETA:  0:02:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 620, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  63% |###########################                | ETA:  0:02:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 640, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  66% |############################               | ETA:  0:02:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 660, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  67% |#############################              | ETA:  0:02:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 680, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  69% |##############################             | ETA:  0:02:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 700, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  72% |##############################             | ETA:  0:01:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 720, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  73% |###############################            | ETA:  0:01:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 740, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  75% |################################           | ETA:  0:01:39\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 760, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  78% |#################################          | ETA:  0:01:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 780, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  79% |##################################         | ETA:  0:01:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 800, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  81% |###################################        | ETA:  0:01:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 820, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  84% |####################################       | ETA:  0:01:05\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 840, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  85% |####################################       | ETA:  0:00:58\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 860, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  87% |#####################################      | ETA:  0:00:49\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 880, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  90% |######################################     | ETA:  0:00:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 900, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  91% |#######################################    | ETA:  0:00:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 920, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  93% |########################################   | ETA:  0:00:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 940, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  96% |#########################################  | ETA:  0:00:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 960, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  97% |########################################## | ETA:  0:00:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 980, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 0:06:51\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2b178d940>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "#following generate sim_nos instance of simulation \n",
    "envs = gym.make('Acrobot-v1')\n",
    "mean_rewards = []\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards, next_states = \\\n",
    "    collect_trajectories(envs, actor, tmax=tmax)  \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "    \n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    for _ in range(SGD_epoch):\n",
    "        L = -1*clipped_surrogate(actor, old_probs, states, actions, rewards, next_states, epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "\n",
    "    epsilon*=0.999\n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "    if(np.mean(total_rewards) == 200):\n",
    "        break\n",
    "    \n",
    "timer.finish()\n",
    "plt.plot(mean_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.forward(state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1061, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3746e-06, 1.7109e-08, 3.1209e-08], device='cuda:0',\n",
      "Reward -1.0 with action 1 with critic baseline tensor([0.1265, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.7743e-13, 7.0716e-13, 6.9596e-13], device='cuda:0',\n",
      "Reward -1.0 with action 2 with critic baseline tensor([0.1296, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1729e-12, 1.2641e-12, 1.2815e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1206, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5463e-05, 5.0799e-08, 1.0961e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1250, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.2775e-08, 1.7981e-09, 3.0179e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1249, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3489e-08, 5.2186e-10, 8.1165e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1242, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.1975e-08, 1.4638e-09, 2.5270e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1236, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.4411e-06, 2.3121e-08, 5.0555e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1239, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0807e-03, 7.7207e-07, 2.2517e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1249, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.5578e-01, 1.9005e-05, 7.1514e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1255, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.6978e-01, 1.3044e-04, 5.6355e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1260, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.9646e-01, 1.6219e-04, 7.0226e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1269, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.8168e-01, 5.1774e-05, 2.0470e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1300, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.6675e-02, 5.5792e-06, 1.8324e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1340, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6892e-04, 5.5483e-07, 1.5139e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1369, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0420e-04, 1.3152e-07, 3.2161e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1392, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0867e-04, 1.2946e-07, 3.1860e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1393, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1457e-03, 5.5530e-07, 1.5433e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1371, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.4819e-02, 6.4905e-06, 2.1960e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1301, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.6782e-01, 8.4597e-05, 3.5174e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1220, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6843e-01, 3.9939e-04, 1.7604e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1150, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6774e-01, 4.1813e-04, 1.8271e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1110, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.7270e-01, 1.0424e-04, 4.2126e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1113, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.8587e-02, 4.2116e-06, 1.3140e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1160, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.0484e-05, 7.5585e-08, 1.7137e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1188, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0603e-07, 2.0622e-09, 3.5535e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1230, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.6772e-09, 2.6292e-10, 3.9265e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1272, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.6576e-09, 3.1805e-10, 4.9315e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1312, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.7890e-07, 3.4225e-09, 6.5149e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1362, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3181e-04, 1.8758e-07, 4.9461e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1397, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.7959e-01, 1.9894e-05, 7.5802e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1397, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8498e-01, 6.7300e-04, 3.0119e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1157, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.8542e-01, 1.8163e-04, 7.5759e-04], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1168, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.9176e-03, 1.9059e-06, 5.4706e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1211, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.6149e-06, 2.0406e-08, 4.0992e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1244, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.1161e-08, 9.6194e-10, 1.5418e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1245, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6867e-09, 4.1588e-10, 6.3841e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1237, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1064e-07, 1.7322e-09, 3.0455e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1236, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5449e-05, 3.6122e-08, 8.2154e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1243, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.2303e-03, 1.3688e-06, 4.1885e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1257, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.4551e-01, 3.2388e-05, 1.2714e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1263, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1281e-01, 1.7798e-04, 7.7959e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1265, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1421e-01, 1.8808e-04, 8.1930e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1274, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.8738e-01, 5.2700e-05, 2.0830e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1306, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.3048e-02, 5.2236e-06, 1.7040e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1346, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1174e-04, 5.3361e-07, 1.4500e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1373, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1548e-04, 1.4010e-07, 3.4405e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1390, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.4305e-04, 1.5417e-07, 3.8433e-07], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1386, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.6470e-03, 7.0100e-07, 1.9816e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1354, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.1293e-02, 7.7965e-06, 2.6714e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1283, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.7341e-01, 8.6900e-05, 3.6129e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1206, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6093e-01, 3.4135e-04, 1.5046e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1143, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.4957e-01, 3.0100e-04, 1.3166e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1112, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.4280e-01, 5.4027e-05, 2.0767e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1124, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.7036e-03, 1.9829e-06, 5.8383e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1169, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1841e-05, 4.1104e-08, 8.9141e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1198, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.9812e-08, 1.5477e-09, 2.6206e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1242, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.1302e-09, 3.0323e-10, 4.6039e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1285, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.9529e-08, 5.5955e-10, 9.1088e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1328, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.4566e-06, 7.9286e-09, 1.6158e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1378, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0970e-03, 4.9795e-07, 1.4184e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1403, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.0935e-01, 4.8387e-05, 1.9814e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1384, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.9008e-01, 9.2883e-04, 4.1685e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1150, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.8476e-01, 8.3328e-05, 3.2486e-04], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1168, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.4629e-04, 6.8347e-07, 1.8058e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1217, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.8725e-07, 8.4193e-09, 1.5793e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1239, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3187e-08, 5.5203e-10, 8.5022e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1242, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.1254e-09, 3.6463e-10, 5.5687e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1235, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7449e-07, 2.2589e-09, 4.0765e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1238, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.6241e-05, 6.0972e-08, 1.4510e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1250, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.4066e-02, 2.5441e-06, 8.1951e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1265, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.5199e-01, 5.5563e-05, 2.2826e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1272, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.3729e-01, 2.3077e-04, 1.0165e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1269, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.2757e-01, 2.1452e-04, 9.3992e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1279, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.8370e-01, 5.2387e-05, 2.0659e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1310, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.8740e-02, 4.7840e-06, 1.5471e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1351, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.3031e-04, 5.0317e-07, 1.3594e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1376, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.2318e-04, 1.4587e-07, 3.5905e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1387, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7894e-04, 1.7793e-07, 4.4815e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1377, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2033e-03, 8.4624e-07, 2.4247e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1337, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.5155e-02, 8.8635e-06, 3.0626e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1267, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.6238e-01, 8.4077e-05, 3.4751e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1194, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.4875e-01, 2.7883e-04, 1.2289e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1138, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1254e-01, 2.0117e-04, 8.6544e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1116, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.8937e-01, 2.7540e-05, 1.0056e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1136, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.9201e-03, 9.9231e-07, 2.7720e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1176, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.3471e-06, 2.4523e-08, 5.1257e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1209, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.4553e-08, 1.2931e-09, 2.1696e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1256, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.3718e-09, 3.8572e-10, 5.9979e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1300, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.4396e-08, 1.0441e-09, 1.7915e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1344, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.8478e-06, 1.8902e-08, 4.1316e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1392, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.0816e-03, 1.3079e-06, 4.0201e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1406, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.4498e-01, 1.0968e-04, 4.7767e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1101, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.6168e-08, 2.0615e-09, 3.2524e-09], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 2 with critic baseline tensor([0.1248, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.5645e-13, 8.3583e-13, 8.3595e-13], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1179, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.3964e-07, 8.7538e-09, 1.6265e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1226, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1097e-09, 1.2455e-10, 1.6858e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1248, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.2895e-10, 2.7925e-11, 3.4609e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1240, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0192e-09, 1.3645e-10, 1.9745e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1255, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1841e-06, 6.9642e-09, 1.4041e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1279, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.8445e-03, 1.0925e-06, 3.3182e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1297, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.5513e-01, 1.1559e-04, 5.0291e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1293, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8869e-01, 8.4958e-04, 3.8090e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1229, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8088e-01, 6.1575e-04, 2.6976e-03], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1237, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.8678e-01, 5.7843e-05, 2.2492e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1279, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.5561e-03, 1.1252e-06, 3.1783e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1339, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3975e-05, 4.1374e-08, 9.0316e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1363, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3637e-06, 8.9346e-09, 1.7491e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1364, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.2044e-06, 1.6953e-08, 3.5414e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1349, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2309e-04, 1.9899e-07, 5.1039e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1314, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.7775e-02, 5.9170e-06, 1.9941e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1257, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.6422e-01, 1.2852e-04, 5.5465e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1181, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7443e-01, 4.7425e-04, 2.0923e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1128, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.5452e-01, 3.3224e-04, 1.4452e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1118, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.0532e-01, 3.8611e-05, 1.4457e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1155, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3801e-03, 8.0934e-07, 2.2266e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1203, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7399e-06, 1.2083e-08, 2.3890e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1250, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5234e-08, 5.6920e-10, 8.9679e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1306, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.0295e-09, 2.5176e-10, 3.7995e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1342, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1163e-08, 1.4209e-09, 2.5063e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1380, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.7106e-05, 6.9401e-08, 1.6845e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1415, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.2964e-01, 1.0926e-05, 3.9610e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1399, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8604e-01, 7.1015e-04, 3.1772e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1099, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.0522e-01, 2.2202e-04, 9.2768e-04], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1124, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.4263e-03, 9.4028e-07, 2.5244e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1191, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7682e-07, 3.2626e-09, 5.6213e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1226, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.5902e-10, 6.9907e-11, 9.0891e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1244, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.2632e-10, 2.6799e-11, 3.3317e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1241, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.3699e-09, 2.1628e-10, 3.2634e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1263, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.0577e-06, 1.4957e-08, 3.2145e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1287, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5238e-02, 2.6036e-06, 8.4830e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1306, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.3205e-01, 2.0946e-04, 9.2554e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1228, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7900e-01, 5.7995e-04, 2.5308e-03], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1239, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.5064e-01, 4.1292e-05, 1.5596e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1281, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3339e-03, 7.5073e-07, 2.0502e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1341, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.7401e-06, 3.0795e-08, 6.5640e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1359, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1655e-06, 8.0777e-09, 1.5691e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1357, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.8290e-06, 1.8494e-08, 3.8908e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1338, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.0279e-04, 2.4144e-07, 6.2886e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1303, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.1857e-02, 7.0399e-06, 2.4051e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1247, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.7101e-01, 1.3376e-04, 5.7813e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1176, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6886e-01, 4.1319e-04, 1.8153e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1130, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.3522e-01, 2.5471e-04, 1.1040e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1127, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.4062e-01, 2.3748e-05, 8.5819e-05], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1172, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.2455e-04, 4.8739e-07, 1.2907e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1215, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1080e-06, 8.9419e-09, 1.7321e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1264, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.6934e-08, 5.9484e-10, 9.4412e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1318, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0386e-08, 3.8889e-10, 6.0940e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1355, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.9487e-07, 2.9533e-09, 5.5283e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1392, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.9991e-04, 1.7195e-07, 4.4837e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1416, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.6951e-01, 2.5969e-05, 1.0058e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1375, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.9088e-01, 9.8890e-04, 4.4481e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1097, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.8314e-01, 8.8522e-05, 3.4264e-04], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1130, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2664e-04, 2.9771e-07, 7.2948e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1205, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.1404e-08, 1.2943e-09, 2.0798e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1227, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2502e-10, 4.3538e-11, 5.4853e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1241, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.4963e-10, 2.8960e-11, 3.6458e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1243, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0675e-08, 3.7282e-10, 5.9030e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1272, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5369e-05, 3.4256e-08, 7.8837e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1296, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.0245e-02, 6.3411e-06, 2.2192e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1312, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6500e-01, 3.5064e-04, 1.5549e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1226, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7564e-01, 5.2529e-04, 2.2840e-03], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1240, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.0615e-01, 2.8228e-05, 1.0318e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1283, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.6413e-04, 4.8655e-07, 1.2818e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1342, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.3250e-06, 2.2549e-08, 4.6876e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1355, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8095e-07, 7.2335e-09, 1.3934e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1349, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.4718e-06, 1.9999e-08, 4.2352e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1326, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.0382e-04, 2.8956e-07, 7.6534e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1292, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.8328e-02, 8.2671e-06, 2.8609e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1238, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.7527e-01, 1.3750e-04, 5.9452e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1172, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6210e-01, 3.5898e-04, 1.5716e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1133, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.0274e-01, 1.8897e-04, 8.0939e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1138, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.2986e-01, 1.4679e-05, 5.1195e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1188, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.1127e-04, 3.1163e-07, 7.9815e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1227, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.9040e-07, 7.1054e-09, 1.3559e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1278, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0940e-08, 6.6528e-10, 1.0691e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1330, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3101e-08, 6.3050e-10, 1.0294e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1367, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8370e-07, 6.2664e-09, 1.2463e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1401, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.2853e-04, 4.2016e-07, 1.1754e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1415, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.6966e-01, 5.7717e-05, 2.3929e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1172, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.1036e-06, 3.5892e-08, 7.3784e-08], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1285, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.8527e-10, 4.3376e-11, 5.3267e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1358, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0736e-12, 1.6454e-12, 1.7264e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1362, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.4948e-11, 6.1007e-12, 6.9260e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1341, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.6448e-08, 1.0168e-09, 1.7630e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1354, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.6578e-03, 1.5521e-06, 4.8372e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1351, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8867e-01, 8.4318e-04, 3.7789e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1021, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.6625e-01, 1.9030e-04, 7.6720e-04], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1116, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.9375e-05, 1.2615e-07, 2.8467e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1229, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.1365e-10, 6.4615e-11, 8.1004e-11], device='cuda:0',\n",
      "Reward -1.0 with action 2 with critic baseline tensor([0.1331, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.2827e-13, 7.5799e-13, 7.7459e-13], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1296, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.6765e-04, 3.1261e-07, 8.0726e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1313, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.7174e-05, 8.3301e-08, 1.9490e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1313, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.4468e-05, 7.6774e-08, 1.8006e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1305, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.5931e-04, 2.2435e-07, 5.7756e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1291, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.4769e-03, 1.3257e-06, 3.9496e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1272, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.9070e-02, 8.4353e-06, 2.9140e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1246, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.8421e-01, 2.9764e-05, 1.1333e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1219, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.4320e-01, 3.6033e-05, 1.3848e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1212, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3735e-01, 1.3679e-05, 4.8349e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1230, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.9936e-03, 2.1621e-06, 6.5697e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1252, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.7508e-04, 2.5948e-07, 6.6634e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1279, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0445e-05, 4.9099e-08, 1.1111e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1311, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.7212e-06, 2.7192e-08, 5.9276e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1339, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.1707e-05, 5.7986e-08, 1.3556e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1358, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.3797e-04, 4.0806e-07, 1.1212e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1363, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.1188e-02, 5.9871e-06, 2.0396e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1351, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.7704e-01, 8.4261e-05, 3.5571e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1289, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7190e-01, 4.2358e-04, 1.8777e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1229, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7978e-01, 5.5723e-04, 2.4692e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1186, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.3990e-01, 2.6043e-04, 1.1403e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1172, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.6393e-01, 2.4887e-05, 9.0071e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1189, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7373e-03, 9.1126e-07, 2.5229e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1231, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1674e-05, 3.8892e-08, 8.4190e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1242, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.3716e-07, 4.6666e-09, 8.6487e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1261, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2550e-07, 2.8350e-09, 5.1381e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1280, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.8106e-06, 9.6733e-09, 1.9637e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1297, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0435e-04, 1.1845e-07, 2.9657e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1311, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.4569e-02, 2.6179e-06, 8.4161e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1315, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.7959e-01, 4.5596e-05, 1.8388e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1308, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.4692e-01, 2.5848e-04, 1.1435e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1273, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6373e-01, 3.5610e-04, 1.5674e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1253, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.0085e-01, 1.7215e-04, 7.4426e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1252, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.6411e-01, 2.3237e-05, 8.4757e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1270, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.3056e-03, 1.8895e-06, 5.6173e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1297, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0964e-04, 2.1980e-07, 5.5182e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1310, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.4666e-05, 6.8451e-08, 1.5779e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1308, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.3633e-05, 7.5575e-08, 1.7722e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1299, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.1665e-04, 2.5371e-07, 6.6024e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1285, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.9640e-03, 1.5863e-06, 4.7975e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1267, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6828e-02, 9.6920e-06, 3.3868e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1244, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.8941e-01, 3.0255e-05, 1.1536e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1220, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.9207e-01, 3.1661e-05, 1.2043e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1217, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7486e-02, 1.0730e-05, 3.7209e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1240, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.2329e-03, 1.6490e-06, 4.9069e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1260, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0659e-04, 2.1505e-07, 5.4477e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1288, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0623e-05, 4.8809e-08, 1.1059e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1319, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.2710e-05, 3.4052e-08, 7.5685e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1345, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.2566e-05, 8.8488e-08, 2.1410e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1361, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7606e-03, 7.0447e-07, 2.0210e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1361, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1199e-01, 1.0235e-05, 3.6339e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1336, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.5697e-01, 1.2065e-04, 5.2195e-04], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1274, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7429e-01, 4.5471e-04, 2.0174e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1217, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7515e-01, 4.8485e-04, 2.1350e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1180, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.8949e-01, 1.6872e-04, 7.1972e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1173, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7868e-02, 1.1877e-05, 4.0476e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1196, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.2847e-04, 4.3138e-07, 1.1263e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1231, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.8092e-06, 2.2044e-08, 4.5747e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1245, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.0528e-07, 3.6569e-09, 6.6751e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1266, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.8862e-07, 3.2446e-09, 5.9688e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1286, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.8437e-06, 1.5293e-08, 3.2301e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1303, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.9508e-04, 2.2614e-07, 5.9709e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1316, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.1695e-02, 5.1499e-06, 1.7474e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1319, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.6147e-01, 7.8193e-05, 3.3046e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1301, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.5987e-01, 3.2208e-04, 1.4229e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1267, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6425e-01, 3.6184e-04, 1.5909e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1249, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.6930e-01, 1.4165e-04, 6.0458e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1251, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5677e-01, 1.5533e-05, 5.4746e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1271, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.1156e-03, 1.2173e-06, 3.4907e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1298, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1979e-04, 1.5463e-07, 3.7755e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1307, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.6089e-05, 5.7058e-08, 1.2975e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1303, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.4161e-05, 7.5817e-08, 1.7805e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1294, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.9670e-04, 2.9144e-07, 7.6771e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1280, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.0509e-03, 1.9139e-06, 5.8806e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1263, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1834e-01, 1.1151e-05, 3.9424e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1242, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.9450e-01, 3.0731e-05, 1.1734e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1222, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.4457e-01, 2.7913e-05, 1.0511e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1224, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.9729e-02, 8.5335e-06, 2.9060e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1247, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.5744e-03, 1.2919e-06, 3.7730e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1268, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.6480e-04, 1.8508e-07, 4.6387e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1297, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2177e-05, 5.0531e-08, 1.1499e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1327, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.9461e-05, 4.4030e-08, 1.0001e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1350, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.2592e-04, 1.3686e-07, 3.4298e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1362, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.0991e-03, 1.1998e-06, 3.5890e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1358, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.1520e-01, 1.6738e-05, 6.1700e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1321, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.0252e-01, 1.6196e-04, 7.0998e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1259, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7453e-01, 4.6009e-04, 2.0393e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1112, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7181e-04, 2.7740e-07, 6.6525e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1233, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.2980e-10, 1.0175e-10, 1.3168e-10], device='cuda:0',\n",
      "Reward -1.0 with action 2 with critic baseline tensor([0.1338, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.5555e-13, 7.9304e-13, 8.0924e-13], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1295, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3419e-04, 2.3937e-07, 6.0261e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1318, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2641e-05, 5.3158e-08, 1.1963e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1315, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.1310e-05, 4.8681e-08, 1.0991e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1302, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5552e-04, 1.6322e-07, 4.0936e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1284, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.7602e-03, 1.1890e-06, 3.5115e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1263, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.9347e-02, 9.1747e-06, 3.1917e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1235, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.5369e-01, 3.5685e-05, 1.3789e-04], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1207, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.9782e-01, 4.1623e-05, 1.6174e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1203, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3316e-01, 1.3481e-05, 4.7577e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1227, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.3811e-03, 1.6983e-06, 5.0596e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1251, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3432e-04, 1.6631e-07, 4.1221e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1284, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.7674e-06, 2.8800e-08, 6.2529e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1321, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.3363e-06, 1.7358e-08, 3.6592e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1354, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3210e-05, 4.7196e-08, 1.0885e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1374, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.5380e-04, 4.7553e-07, 1.3258e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1377, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1624e-01, 1.0408e-05, 3.7117e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1352, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1970e-01, 1.8529e-04, 8.2046e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1277, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8557e-01, 7.0161e-04, 3.1412e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1209, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8745e-01, 8.0785e-04, 3.5980e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1164, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6008e-01, 3.6433e-04, 1.5890e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1150, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.5231e-01, 2.4843e-05, 8.9247e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1172, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.6251e-04, 5.1606e-07, 1.3561e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1218, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2058e-06, 1.4063e-08, 2.7943e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1233, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.1207e-08, 1.3762e-09, 2.3113e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1256, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.8030e-08, 9.2404e-10, 1.5350e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1274, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.6058e-07, 4.5603e-09, 8.7583e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1295, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.1519e-05, 9.2098e-08, 2.2717e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1316, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2633e-02, 3.4276e-06, 1.1310e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1323, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.9666e-01, 8.8379e-05, 3.7908e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1312, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7523e-01, 4.6173e-04, 2.0535e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1272, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8173e-01, 5.9714e-04, 2.6541e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1248, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.4945e-01, 2.8942e-04, 1.2682e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1247, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.9810e-01, 3.4606e-05, 1.2978e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1265, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.1392e-03, 1.8927e-06, 5.6011e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1296, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.2184e-04, 1.5904e-07, 3.8745e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1314, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5437e-05, 4.1696e-08, 9.2105e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1309, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.9833e-05, 4.6362e-08, 1.0439e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1295, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.8385e-04, 1.8079e-07, 4.5769e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1278, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.9152e-03, 1.4046e-06, 4.2080e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1257, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0796e-01, 1.0451e-05, 3.6764e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1234, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.5709e-01, 3.6047e-05, 1.3947e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1210, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.4496e-01, 3.6476e-05, 1.4031e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1210, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.5135e-02, 1.0620e-05, 3.6792e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1235, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.6246e-03, 1.3176e-06, 3.8504e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1260, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0687e-04, 1.4271e-07, 3.4997e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1294, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6475e-06, 3.0203e-08, 6.5940e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1330, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.0404e-06, 2.3251e-08, 5.0239e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1360, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.1092e-05, 7.7194e-08, 1.8523e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1376, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.5069e-03, 8.7333e-07, 2.5541e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1374, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.4722e-01, 1.8551e-05, 6.9156e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1335, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.4582e-01, 2.5320e-04, 1.1215e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1260, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8580e-01, 7.1488e-04, 3.1973e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1196, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8464e-01, 7.0128e-04, 3.1135e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1157, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.2390e-01, 2.2696e-04, 9.7948e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1151, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.7789e-02, 1.0512e-05, 3.5230e-05], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1181, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7298e-04, 2.2167e-07, 5.4541e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1219, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.4371e-07, 7.5817e-09, 1.4396e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1237, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.3685e-08, 1.0889e-09, 1.8042e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1261, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.4348e-08, 1.1297e-09, 1.9160e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1281, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3729e-06, 7.9043e-09, 1.5915e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1304, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3858e-04, 1.9524e-07, 5.1204e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1324, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.3808e-02, 7.4424e-06, 2.6118e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1328, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.0178e-01, 1.5679e-04, 6.9105e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1305, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8056e-01, 5.5724e-04, 2.4876e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1265, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8202e-01, 6.0732e-04, 2.6972e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1243, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.3490e-01, 2.4049e-04, 1.0529e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1244, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3832e-01, 2.1896e-05, 7.8960e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1265, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.7313e-03, 1.1432e-06, 3.2457e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1297, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.3087e-05, 1.0526e-07, 2.4817e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1309, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0753e-05, 3.3117e-08, 7.1900e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1302, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.9081e-05, 4.5046e-08, 1.0134e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1289, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.2439e-04, 2.0417e-07, 5.2264e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1272, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.5764e-03, 1.6833e-06, 5.1222e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1253, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3160e-01, 1.2013e-05, 4.2766e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1233, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.6371e-01, 3.6693e-05, 1.4223e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1214, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.9762e-01, 3.2307e-05, 1.2313e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1219, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.9209e-02, 8.5375e-06, 2.9077e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1244, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.5653e-03, 1.0547e-06, 3.0304e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1270, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.0512e-05, 1.2738e-07, 3.0998e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1304, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1289e-05, 3.2944e-08, 7.2535e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1339, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1926e-05, 3.2025e-08, 7.1063e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1365, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1355e-04, 1.2715e-07, 3.1753e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1377, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.3531e-03, 1.5711e-06, 4.8106e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1370, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.2653e-01, 3.1157e-05, 1.2078e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1317, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.5987e-01, 3.2081e-04, 1.4195e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1244, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.8534e-01, 7.0249e-04, 3.1376e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1185, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7965e-01, 5.7719e-04, 2.5408e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1152, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.2302e-01, 1.2440e-04, 5.1167e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1154, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0593e-02, 4.4299e-06, 1.3853e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1192, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.8901e-05, 9.9766e-08, 2.3083e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1221, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.7248e-07, 4.4611e-09, 8.1551e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1242, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.7067e-08, 9.5956e-10, 1.5820e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1266, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1089e-08, 1.5274e-09, 2.6656e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1289, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.7471e-06, 1.4691e-08, 3.1168e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1312, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.3346e-04, 4.2672e-07, 1.1922e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1329, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.1233e-01, 1.6002e-05, 5.9655e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1331, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.4519e-01, 2.4898e-04, 1.1021e-03], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1108, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.8978e-08, 1.9826e-09, 3.0850e-09], device='cuda:0',\n",
      "Reward -1.0 with action 1 with critic baseline tensor([0.1292, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3456e-13, 3.7688e-13, 3.6837e-13], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1296, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.1576e-12, 3.7611e-12, 3.9757e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1297, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7727e-12, 1.4027e-12, 1.4746e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1260, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3464e-10, 3.4084e-11, 4.4508e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1261, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.4972e-06, 1.3525e-08, 2.8921e-08], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1281, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7315e-01, 1.3421e-05, 4.9793e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1130, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7441e-01, 5.4359e-04, 2.3323e-03], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1185, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.8370e-02, 5.8486e-06, 1.8488e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1281, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7768e-06, 1.3039e-08, 2.5398e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1353, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.5634e-09, 1.9256e-10, 2.7366e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1387, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.3723e-10, 8.1851e-11, 1.1174e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1385, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.4002e-08, 1.1482e-09, 1.9757e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1386, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.0599e-04, 2.2813e-07, 6.0365e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1366, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.2126e-01, 9.9913e-05, 4.3171e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1012, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.3476e-01, 2.9827e-04, 1.2663e-03], device='cuda:0',gmoidBackward>)  \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1065, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0320e-03, 8.1934e-07, 2.1676e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1171, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0734e-08, 5.9455e-10, 8.9236e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1263, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.4623e-12, 3.2724e-12, 3.4762e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1322, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0535e-12, 8.8220e-13, 9.2281e-13], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1320, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.1460e-11, 1.4001e-11, 1.7278e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1339, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.5686e-07, 5.8418e-09, 1.1798e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1394, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.3100e-01, 1.0873e-05, 3.9781e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1058, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6268e-01, 4.5896e-04, 1.9606e-03], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1112, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.0927e-04, 4.7949e-07, 1.1926e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1217, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.7447e-09, 2.6030e-10, 3.6007e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1293, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.7811e-12, 2.2933e-12, 2.3922e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1291, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7881e-12, 1.3886e-12, 1.4640e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1253, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.3930e-10, 4.9965e-11, 6.7514e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1261, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.4673e-06, 2.5128e-08, 5.6572e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1285, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.6725e-01, 2.5456e-05, 9.9446e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1137, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7006e-01, 4.8767e-04, 2.0918e-03], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1193, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.7132e-02, 4.2235e-06, 1.3004e-05], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1288, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1774e-06, 1.0024e-08, 1.9125e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1359, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.3724e-09, 1.8191e-10, 2.5760e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1385, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1434e-09, 9.8921e-11, 1.3718e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1382, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.1512e-07, 1.6601e-09, 2.9410e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1379, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.8992e-04, 3.4542e-07, 9.4394e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1355, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.7516e-01, 1.3252e-04, 5.8038e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1010, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([8.7169e-01, 1.8729e-04, 7.6302e-04], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1087, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.2613e-04, 3.9118e-07, 9.7954e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1179, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([5.1013e-09, 3.6453e-10, 5.2874e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1277, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.2887e-12, 3.1024e-12, 3.3014e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1327, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.8414e-12, 1.3425e-12, 1.4254e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1325, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0637e-10, 3.0131e-11, 3.9537e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1353, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.4768e-06, 1.5392e-08, 3.3571e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1405, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.8745e-01, 2.7004e-05, 1.0614e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1054, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1368e-01, 2.5915e-04, 1.0714e-03], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1117, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.7442e-05, 1.6982e-07, 3.8865e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1228, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.3637e-10, 1.1282e-10, 1.4645e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1291, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.6236e-12, 1.4838e-12, 1.5311e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1285, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.9778e-12, 1.4764e-12, 1.5645e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1250, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.1086e-10, 7.8173e-11, 1.0985e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1265, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.7904e-05, 4.9268e-08, 1.1727e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1289, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([6.1636e-01, 4.8970e-05, 2.0208e-04], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with critic baseline tensor([0.1144, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.6447e-01, 4.3284e-04, 1.8560e-03], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1200, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.7756e-03, 2.9585e-06, 8.8461e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1295, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.6037e-07, 7.5860e-09, 1.4157e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1364, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.1504e-09, 1.6971e-10, 2.3921e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1381, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.5252e-09, 1.1788e-10, 1.6587e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1377, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0073e-07, 2.3543e-09, 4.2879e-09], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1372, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0902e-03, 5.0932e-07, 1.4345e-06], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1343, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([9.0935e-01, 1.6846e-04, 7.4594e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1016, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.2780e-01, 1.0536e-04, 4.1083e-04], device='cuda:0',gmoidBackward>) \n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1103, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([1.0929e-04, 1.9482e-07, 4.6279e-07], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1188, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.7140e-09, 2.3974e-10, 3.3794e-10], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1291, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([4.5307e-12, 3.1672e-12, 3.3840e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1332, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([3.4818e-12, 2.1732e-12, 2.3477e-12], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1332, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.5511e-10, 6.7777e-11, 9.4980e-11], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1365, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([2.0885e-05, 4.0504e-08, 9.5367e-08], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1416, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([7.0781e-01, 6.4079e-05, 2.7167e-04], device='cuda:0',\n",
      "Reward -1.0 with action 0 with critic baseline tensor([0.1078, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>) tensor([0.9994, 0.0064, 0.0373], device='cuda:0', grad_fn=<SigmoidBackward>) "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    for i in range(100):\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = prob.argmax()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with critic baseline {} {}'.format(reward, action, action_baseline, prob), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
