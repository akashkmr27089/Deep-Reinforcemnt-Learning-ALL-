{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (6,)\n",
      "Number of actions:  3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (final): Sigmoid()\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Critic(nn.Module):  #gives score of how bad or good the action is \n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "#         x = self.fc1(state)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = torch.tanh(x)   #using tanh for giving score of how good is action \n",
    "#         return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)   #using tanh for giving score of how good is action \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Actor(nn.Module):     #Policy Network\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32,action_size)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.final(x)    #using sigmoid in an action \n",
    "        return x    \n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "actor = Actor(6,3,12).to(device)\n",
    "critic = Critic(6,3,12).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=1e-4)\n",
    "print(actor)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward -1.0 with action 0 with score tensor([0.1019, 0.0000, 0.0000], device='cuda:0', grad_fn=<ReluBackward0>)        "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    for i in range(100):\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action = prob.argmax()\n",
    "        prob = max(prob)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Making of Network using ppo Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards, next_states,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01,\n",
    "                     gamma = 0.1):\n",
    "\n",
    "    states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "    next_states = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards_te = np.multiply(rewards, discount).reshape(len(rewards),1)\n",
    "    rewards_future = rewards_te[::-1].cumsum(axis=0)[::-1]\n",
    "    actions = np.array(actions, dtype=np.int8)\n",
    "    actions_final = torch.LongTensor(actions.reshape(len(actions),1))\n",
    "    \n",
    "#     # adding contribution of actor\n",
    "#     f1 = critic.forward(next_states).argmax(1).reshape(len(next_states),1)\n",
    "#     f2 = torch.LongTensor(f1.cpu().reshape(f1.size()[0],1))\n",
    "#     f3 = torch.gather(f1,1,f2.to(device))\n",
    "    \n",
    "# #     f1 = critic.forward(states).argmax(1).reshape(len(next_states),1)\n",
    "# #     f2 = torch.LongTensor(f1.cpu().reshape(f1.size()[0],1))\n",
    "# #     f4 = torch.gather(f1,1,f2.to(device))\n",
    "#     f1 = critic.forward(states)\n",
    "#     f4 = torch.gather(f1,1,actions_final.to(device))\n",
    "    \n",
    "#     rewards_future = rewards_future + gamma*f3.detach().cpu().numpy() - f4.detach().cpu().numpy()\n",
    "#     ##end\n",
    "    mean = np.mean(rewards_future, axis = 0)\n",
    "    std = np.std(rewards_future, axis = 0)\n",
    "    rewards_normalized = (rewards_future - mean)/std\n",
    "    \n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device).reshape(len(old_probs),1)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    \n",
    "    g = actor.forward(states)\n",
    "    new_probs = torch.gather(g,1,actions_final.to(device))\n",
    "    \n",
    "    ratio = new_probs/old_probs\n",
    "#     # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "    \n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_baseline(next_state, reward, state):\n",
    "    next_state = torch.from_numpy(np.array(next_state)).to(device).float()\n",
    "    reward = torch.from_numpy(np.array(reward)).to(device)\n",
    "    state = torch.from_numpy(np.array(state)).to(device).float()\n",
    "    Loss = F.mse_loss(critic.forward(state), reward + critic.forward(next_state))\n",
    "    optimizer_critic.zero_grad()\n",
    "    Loss.backward()\n",
    "    optimizer_critic.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200):\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "    next_states = []\n",
    "    \n",
    "    for _ in range(tmax):\n",
    "        prob = actor(torch.from_numpy(state).float().to(device))   #for converting state to torch variable \n",
    "        prob_new = max(prob)\n",
    "        probs.append(prob_new)\n",
    "        states.append(state)\n",
    "        action = prob.argmax()\n",
    "        next_state, reward, done , _ = env.step(action)\n",
    "#         update_baseline(next_state, reward,state)\n",
    "        next_states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return probs, states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, states, actions, rewards, next_states = collect_trajectories(env, actor, tmax=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 200\n",
    "SGD_epoch = 4\n",
    "episode = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                           | ETA:  0:06:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                          | ETA:  0:06:36\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                         | ETA:  0:06:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                        | ETA:  0:06:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |####                                       | ETA:  0:06:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                      | ETA:  0:06:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                      | ETA:  0:06:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 140, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  15% |######                                     | ETA:  0:05:54\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 160, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                    | ETA:  0:05:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 180, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  0:05:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  21% |#########                                  | ETA:  0:05:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 220, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  23% |##########                                 | ETA:  0:05:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 240, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |###########                                | ETA:  0:05:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 260, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  27% |###########                                | ETA:  0:05:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  29% |############                               | ETA:  0:04:52\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                              | ETA:  0:04:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 320, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  34% |##############                             | ETA:  0:04:35\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 340, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  35% |###############                            | ETA:  0:04:27\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 360, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  37% |################                           | ETA:  0:04:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 380, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  0:04:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  41% |#################                          | ETA:  0:04:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 420, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  43% |##################                         | ETA:  0:03:52\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 440, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  46% |###################                        | ETA:  0:03:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 460, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  48% |####################                       | ETA:  0:03:36\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 480, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  49% |#####################                      | ETA:  0:03:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  51% |######################                     | ETA:  0:03:20\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 520, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  53% |#######################                    | ETA:  0:03:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 540, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  55% |########################                   | ETA:  0:03:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 560, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  58% |########################                   | ETA:  0:02:54\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 580, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  59% |#########################                  | ETA:  0:02:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 600, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  61% |##########################                 | ETA:  0:02:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 620, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  63% |###########################                | ETA:  0:02:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 640, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  66% |############################               | ETA:  0:02:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 660, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  67% |#############################              | ETA:  0:02:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 680, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  69% |##############################             | ETA:  0:02:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 700, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  72% |##############################             | ETA:  0:01:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 720, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  73% |###############################            | ETA:  0:01:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 740, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  75% |################################           | ETA:  0:01:39\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 760, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  78% |#################################          | ETA:  0:01:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 780, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  79% |##################################         | ETA:  0:01:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 800, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  81% |###################################        | ETA:  0:01:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 820, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  84% |####################################       | ETA:  0:01:05\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 840, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  85% |####################################       | ETA:  0:00:58\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 860, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  87% |#####################################      | ETA:  0:00:49\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 880, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  90% |######################################     | ETA:  0:00:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 900, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  91% |#######################################    | ETA:  0:00:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 920, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  93% |########################################   | ETA:  0:00:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 940, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  96% |#########################################  | ETA:  0:00:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 960, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  97% |########################################## | ETA:  0:00:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 980, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 0:06:51\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, score: -200.000000\n",
      "-200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2b178d940>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "#following generate sim_nos instance of simulation \n",
    "envs = gym.make('Acrobot-v1')\n",
    "mean_rewards = []\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards, next_states = \\\n",
    "    collect_trajectories(envs, actor, tmax=tmax)  \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "    \n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    for _ in range(SGD_epoch):\n",
    "        L = -1*clipped_surrogate(actor, old_probs, states, actions, rewards, next_states, epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "\n",
    "    epsilon*=0.999\n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "    if(np.mean(total_rewards) == 200):\n",
    "        break\n",
    "    \n",
    "timer.finish()\n",
    "plt.plot(mean_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.forward(state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    for i in range(100):\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = prob.argmax()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with critic baseline {} {}'.format(reward, action, action_baseline, prob), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
