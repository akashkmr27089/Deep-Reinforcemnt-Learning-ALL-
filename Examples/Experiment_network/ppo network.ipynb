{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Critic(nn.Module):  #gives score of how bad or good the action is \n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.tanh(x)   #using tanh for giving score of how good is action \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Actor(nn.Module):     #Policy Network\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32,action_size)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.final(x)    #using sigmoid in an action \n",
    "        return x    \n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "actor = Actor(4,1,12).to(device)\n",
    "critic = Critic(4,1,12).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "print(actor)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Making of Network using ppo Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "#     discount = discount**np.arange(len(rewards))\n",
    "#     rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "#     # convert rewards to future rewards\n",
    "#     rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "#     mean = np.mean(rewards_future, axis=1)\n",
    "#     std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "#     rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards_te = np.multiply(rewards, discount).reshape(len(rewards),1)\n",
    "    rewards_future = rewards_te[::-1].cumsum(axis=0)[::-1]\n",
    "    mean = np.mean(rewards_future, axis = 0)\n",
    "    std = np.std(rewards_future, axis = 0)\n",
    "    rewards_normalized = (rewards_future - mean)/std\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device).reshape(len(actions),1)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device).reshape(len(old_probs),1)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = policy.forward(states).reshape(states.size()[0],1)\n",
    "    new_probs = torch.where(actions == 1, new_probs, 1.0-new_probs)\n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "#     # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200):\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "    \n",
    "    for _ in range(tmax):\n",
    "        prob = actor(torch.from_numpy(state).float().to(device))   #for converting state to torch variable \n",
    "        probs.append(prob)\n",
    "        states.append(state)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done , _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return probs, states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 200\n",
    "SGD_epoch = 4\n",
    "episode = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "#following generate sim_nos instance of simulation \n",
    "envs = gym.make('CartPole-v1')\n",
    "mean_rewards = []\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "    collect_trajectories(envs, actor, tmax=tmax)  \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    for _ in range(SGD_epoch):\n",
    "        L = -1*clipped_surrogate(actor, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "\n",
    "    epsilon*=0.999\n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "    if(np.mean(total_rewards) == 200):\n",
    "        break\n",
    "    \n",
    "timer.finish()\n",
    "plt.plot(mean_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), './weights_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
