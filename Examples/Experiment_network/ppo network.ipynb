{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4,)\n",
      "Number of actions:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (final): Sigmoid()\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Critic(nn.Module):  #gives score of how bad or good the action is \n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.tanh(x)   #using tanh for giving score of how good is action \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Actor(nn.Module):     #Policy Network\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed= 12):\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32,action_size)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.final(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.final(x)    #using sigmoid in an action \n",
    "        return x    \n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "actor = Actor(4,1,12).to(device)\n",
    "critic = Critic(4,1,12).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "print(actor)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 1.0 with action 0 with score tensor([-0.1284], device='cuda:0', grad_fn=<TanhBackward>) "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Making of Network using ppo Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "#     discount = discount**np.arange(len(rewards))\n",
    "#     rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "#     # convert rewards to future rewards\n",
    "#     rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "#     mean = np.mean(rewards_future, axis=1)\n",
    "#     std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "#     rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards_te = np.multiply(rewards, discount).reshape(len(rewards),1)\n",
    "    rewards_future = rewards_te[::-1].cumsum(axis=0)[::-1]\n",
    "    mean = np.mean(rewards_future, axis = 0)\n",
    "    std = np.std(rewards_future, axis = 0)\n",
    "    rewards_normalized = (rewards_future - mean)/std\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device).reshape(len(actions),1)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device).reshape(len(old_probs),1)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = policy.forward(states).reshape(states.size()[0],1)\n",
    "    new_probs = torch.where(actions == 1, new_probs, 1.0-new_probs)\n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "    print(ratio, rewards)\n",
    "\n",
    "#     # clipped function\n",
    "#     clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "#     print(clip)\n",
    "#     clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "    \n",
    "    clipped_surrogate = ratio*(rewards)\n",
    "    print(rewards.size(), clipped_surrogate.size(), ratio.size())\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200):\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "    \n",
    "    for _ in range(tmax):\n",
    "        prob = actor(torch.from_numpy(state).float().to(device))   #for converting state to torch variable \n",
    "        probs.append(prob)\n",
    "        states.append(state)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done , _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return probs, states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 200\n",
    "SGD_epoch = 4\n",
    "episode = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4804],\n",
      "        [1.4929],\n",
      "        [1.5024],\n",
      "        [1.5098],\n",
      "        [1.5064],\n",
      "        [1.5004],\n",
      "        [1.4942],\n",
      "        [1.4891]], device='cuda:0', grad_fn=<DivBackward0>) tensor([[ 1.5352],\n",
      "        [ 1.0922],\n",
      "        [ 0.6513],\n",
      "        [ 0.2127],\n",
      "        [-0.2237],\n",
      "        [-0.6579],\n",
      "        [-1.0900],\n",
      "        [-1.5199]], device='cuda:0')\n",
      "torch.Size([8, 1]) torch.Size([8, 1]) torch.Size([8, 1])\n",
      "tensor(-0.0059, device='cuda:0', grad_fn=<NegBackward>)\n",
      "tensor([[1.4808],\n",
      "        [1.4932],\n",
      "        [1.5026],\n",
      "        [1.5099],\n",
      "        [1.5064],\n",
      "        [1.5004],\n",
      "        [1.4940],\n",
      "        [1.4888]], device='cuda:0', grad_fn=<DivBackward0>) tensor([[ 1.5352],\n",
      "        [ 1.0922],\n",
      "        [ 0.6513],\n",
      "        [ 0.2127],\n",
      "        [-0.2237],\n",
      "        [-0.6579],\n",
      "        [-1.0900],\n",
      "        [-1.5199]], device='cuda:0')\n",
      "torch.Size([8, 1]) torch.Size([8, 1]) torch.Size([8, 1])\n",
      "tensor(-0.0061, device='cuda:0', grad_fn=<NegBackward>)\n",
      "tensor([[1.4811],\n",
      "        [1.4934],\n",
      "        [1.5028],\n",
      "        [1.5100],\n",
      "        [1.5064],\n",
      "        [1.5003],\n",
      "        [1.4938],\n",
      "        [1.4885]], device='cuda:0', grad_fn=<DivBackward0>) tensor([[ 1.5352],\n",
      "        [ 1.0922],\n",
      "        [ 0.6513],\n",
      "        [ 0.2127],\n",
      "        [-0.2237],\n",
      "        [-0.6579],\n",
      "        [-1.0900],\n",
      "        [-1.5199]], device='cuda:0')\n",
      "torch.Size([8, 1]) torch.Size([8, 1]) torch.Size([8, 1])\n",
      "tensor(-0.0063, device='cuda:0', grad_fn=<NegBackward>)\n",
      "tensor([[1.4815],\n",
      "        [1.4937],\n",
      "        [1.5030],\n",
      "        [1.5101],\n",
      "        [1.5064],\n",
      "        [1.5001],\n",
      "        [1.4936],\n",
      "        [1.4882]], device='cuda:0', grad_fn=<DivBackward0>) tensor([[ 1.5352],\n",
      "        [ 1.0922],\n",
      "        [ 0.6513],\n",
      "        [ 0.2127],\n",
      "        [-0.2237],\n",
      "        [-0.6579],\n",
      "        [-1.0900],\n",
      "        [-1.5199]], device='cuda:0')\n",
      "torch.Size([8, 1]) torch.Size([8, 1]) torch.Size([8, 1])\n",
      "tensor(-0.0066, device='cuda:0', grad_fn=<NegBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d8b9fe86a0>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAQ3ElEQVR4nO3df6zddX3H8edr7QgUdVR6/UFxljmFOaKMnTlCXHQpKDClspmsbswFXWqTRYTFTTOy+Yf/TCGZMeqahj+Q6DABy9ycTjazzWQK7raUAiIKdlTaCgdNYBMzKbz3xz1dDqfn3nPOvef2th+ej+SbnvP9vM85709O8sq3n9NvP6kqJEnHv59Z6QYkSdNhoEtSIwx0SWqEgS5JjTDQJakRq1fqg9etW1cbNmxYqY+XpOPSzp07H6uqmWFjKxboGzZsYHZ2dqU+XpKOS0kemm/MJRdJaoSBLkmNMNAlqREGuiQ1wkCXpEaMFehJrk5yb5J7ktyU5MR56n4tydNJ3j7dNiVJo4wM9CTrgSuBTlWdDawCNg+pWwV8BPjKtJuUJI027pLLauCkJKuBNcCBITXvBT4PPDql3iRJExgZ6FW1H7gO2AccBB6vqtv6a3pX8ZcB2xZ6ryRbkswmme12u4vvWpJ0hHGWXNYCm4AzgNOAk5NcPlD2MeADVfX0Qu9VVdurqlNVnZmZoXeuSpIWaZxb/y8A9lZVFyDJDuB84DN9NR3gc0kA1gGXJDlUVX835X4lSfMYJ9D3AeclWQP8BNgIPOs/YamqMw4/TnID8EXDXJKOrnHW0O8AbgF2AXf3XrM9ydYkW5e5P0nSmLJSm0R3Op3yf1uUpMkk2VlVnWFj3ikqSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrEWIGe5Ook9ya5J8lNSU4cGN+UZE+S3Ulmk7x+edqVJM1nZKAnWQ9cCXSq6mxgFbB5oOyrwGur6hzgXcD1025UkrSwcTaJPlx3UpKngDXAgf7BqvqfvqcnAyuzr50kPYeNs0n0fuA6YB9wEHi8qm4brEtyWZJvA//I3FX6EZJs6S3JzHa73aV1Lkl6lnGWXNYCm4AzgNOAk5NcPlhXVbdW1VnA24APD3uvqtpeVZ2q6szMzCytc0nSs4zzo+gFwN6q6lbVU8AO4Pz5iqvqa8ArkqybUo+SpDGME+j7gPOSrEkSYCNwX39Bkl/sjZHkXOAE4IfTblaSNL+RP4pW1R1JbgF2AYeAO4HtSbb2xrcBvwO8s/ej6U+A360qfxiVpKMoK5W7nU6nZmdnV+SzJel4lWRnVXWGjXmnqCQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEWMFepKrk9yb5J4kNyU5cWD895Ps6R1fT/La5WlXkjSfkYGeZD1wJdCpqrOBVcDmgbK9wBuq6jXAh4Ht025UkrSwkXuK9tWd1NszdA1woH+wqr7e9/R24PTptCdJGtfIK/Sq2g9cB+wDDgKPV9VtC7zk3cCXhw0k2ZJkNslst9tdTL+SpHmMs+SyFtgEnAGcBpyc5PJ5an+TuUD/wLDxqtpeVZ2q6szMzCy+a0nSEcb5UfQCYG9VdavqKWAHcP5gUZLXANcDm6rqh9NtU5I0yjiBvg84L8maJAE2Avf1FyT5eeaC/g+q6jvTb1OSNMrIH0Wr6o4ktwC7gEPAncD2JFt749uAvwROBT41l/kcqqrOsnUtSTpCqmpFPrjT6dTs7OyKfLYkHa+S7Jzvgtk7RSWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrEWIGe5Ook9ya5J8lNSU4cGD8ryTeS/G+S9y9Pq5KkhYyzSfR64EqgU1VnA6uAzQNlP+rVXDf1DiVJYxl3yWU1cFKS1cAa4ED/YFU9WlX/CTw15f4kSWMaGehVtZ+5K+99wEHg8aq6bTEflmRLktkks91udzFvIUmaxzhLLmuBTcAZwGnAyUkuX8yHVdX2qupUVWdmZmYxbyFJmsc4Sy4XAHurqltVTwE7gPOXty1J0qTGCfR9wHlJ1iQJsBG4b3nbkiRNavWogqq6I8ktwC7gEHAnsD3J1t74tiQvAWaBFwDPJLkKeHVVPbF8rUuS+o0MdICq+hDwoYHT2/rGfwCcPsW+JEkT8k5RSWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1Ijxgr0JFcnuTfJPUluSnLiwHiSfDzJA0n2JDl3edqVJM1nZKAnWQ9cCXSq6mxgFbB5oOxi4JW9YwvwN1PuU5I0wrhLLquBk5KsBtYABwbGNwE31pzbgVOSvHSKfUqSRhgZ6FW1H7gO2AccBB6vqtsGytYD3+97/nDv3LMk2ZJkNslst9tdfNeSpCOMs+Sylrkr8DOA04CTk1w+WDbkpXXEiartVdWpqs7MzMxi+pUkzWOcJZcLgL1V1a2qp4AdwPkDNQ8DL+t7fjpHLstIkpbROIG+DzgvyZokATYC9w3U/D3wzt6/djmPuWWZg1PuVZK0gNWjCqrqjiS3ALuAQ8CdwPYkW3vj24AvAZcADwBPAlcsW8eSpKFSdcRS91HR6XRqdnZ2RT5bko5XSXZWVWfYmHeKSlIjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaMc4m0Wcm2d13PJHkqoGatUluTbInyTeTnL18LUuShhlnC7r7gXMAkqwC9gO3DpT9ObC7qi5LchbwSeb2HpUkHSWTLrlsBB6sqocGzr8a+CpAVX0b2JDkxVPoT5I0pkkDfTNw05DzdwG/DZDkdcDLgdMHi5JsSTKbZLbb7U7aqyRpAWMHepITgEuBm4cM/xWwNslu4L3AncChwaKq2l5VnarqzMzMLLJlSdIwI9fQ+1wM7KqqRwYHquoJ4AqAJAH29g5J0lEyyZLLOxi+3EKSU3pX8AB/BHytF/KSpKNkrCv0JGuAC4H39J3bClBV24BfAm5M8jTwLeDd029VkrSQsQK9qp4ETh04t63v8TeAV063NUnSJLxTVJIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEaMDPQkZybZ3Xc8keSqgZqfS/IPSe5Kcm+SK5avZUnSMCM3uKiq+4FzAJKsAvYDtw6U/THwrap6a5IZ4P4kn62qn067YUnScJMuuWwEHqyqhwbOF/D83gbRzwN+BByaQn+SpDFNGuibGb5R9CeY21f0AHA38L6qemawKMmWJLNJZrvd7sTNSpLmN3agJzkBuBS4ecjwm4HdwGnMLc98IskLBouqantVdaqqMzMzs8iWJUnDTHKFfjGwq6oeGTJ2BbCj5jwA7AXOmkaDkqTxTBLo72D4cgvAPubW10nyYuBM4HtLa02SNImR/8oFIMka4ELgPX3ntgJU1Tbgw8ANSe4GAnygqh6bfruSpPmMFehV9SRw6sC5bX2PDwBvmm5rkqRJeKeoJDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRIwM9yZlJdvcdTyS5aqDmT/vG70nydJIXLl/bkqRBI3csqqr7gXMAkqwC9gO3DtRcC1zbq3krcHVV/Wjq3UqS5jXpkstG4MGqemiBmoU2k5YkLZNJA30zC4R1bzPpi4DPzzO+JclsktlutzvhR0uSFjJ2oCc5AbgUuHmBsrcC/zHfcktVba+qTlV1ZmZmJutUkrSgSa7QLwZ2VdUjC9QseAUvSVo+kwT6gmvjSX4OeAPwhaU2JUma3FiB3lsbvxDY0Xdua5KtfWWXAbdV1Y+n26IkaRwj/9kiQFU9CZw6cG7bwPMbgBum1ZgkaTLeKSpJjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJasTIQE9yZpLdfccTSa4aUvfG3vi9Sf59edqVJM1n5I5FVXU/cA5AklXAfuDW/pokpwCfAi6qqn1JXrQMvUqSFjDpkstG4MGqemjg/O8BO6pqH0BVPTqN5iRJ45s00DcDNw05/ypgbZJ/S7IzyTuHvTjJliSzSWa73e6kvUqSFjB2oCc5AbgUuHnI8GrgV4HfAt4M/EWSVw0WVdX2qupUVWdmZmaRLUuShhm5ht7nYmBXVT0yZOxh4LGq+jHw4yRfA14LfGcKPUqSxjDJkss7GL7cAvAF4DeSrE6yBvh14L6lNidJGt9YV+i9kL4QeE/fua0AVbWtqu5L8k/AHuAZ4PqqumcZ+pUkzWOsQK+qJ4FTB85tG3h+LXDt9FqTJE3CO0UlqREGuiQ1wkCXpEYY6JLUiFTVynxw0gUG/wuB48E64LGVbuIoc87te67NF47fOb+8qobembligX68SjJbVZ2V7uNocs7te67NF9qcs0suktQIA12SGmGgT277SjewApxz+55r84UG5+wauiQ1wit0SWqEgS5JjTDQh0jywiT/nOS7vT/XzlN3UZL7kzyQ5INDxt+fpJKsW/6uF2+p801ybZJvJ9mT5NbeHrPHpDG+syT5eG98T5Jzx33tsWqxc07ysiT/muS+3ubv7zv63S/OUr7n3viqJHcm+eLR63oKqspj4AA+Cnyw9/iDwEeG1KwCHgR+ATgBuAt4dd/4y4CvMHfz1LqVntNyzhd4E7C69/gjw15/LByjvrNezSXAl4EA5wF3jPvaY/FY4pxfCpzbe/x85jasaXrOfeN/Avwt8MWVns8kh1fow20CPt17/GngbUNqXgc8UFXfq6qfAp/rve6wvwb+DDgefnVe0nyr6raqOtSrux04fZn7XaxR3xm95zfWnNuBU5K8dMzXHosWPeeqOlhVuwCq6r+Z27Rm/dFsfpGW8j2T5HTmttO8/mg2PQ0G+nAvrqqDAL0/XzSkZj3w/b7nD/fOkeRSYH9V3bXcjU7JkuY74F3MXfkci8aZw3w1487/WLOUOf+/JBuAXwHumHqH07fUOX+MuYuxZ5arweUyyZ6iTUnyL8BLhgxdM+5bDDlXvd2drmFuGeKYsVzzHfiMa4BDwGcn6+6oGTmHBWrGee2xaClznhtMngd8Hriqqp6YYm/LZdFzTvIW4NGq2pnkjVPvbJk9ZwO9qi6YbyzJI4f/ytn7a9ijQ8oeZm6d/LDTgQPAK4AzgLuSHD6/K8nrquoHU5vAhJZxvoff4w+BtwAbq7cIeQxacA4jak4Y47XHoqXMmSQ/y1yYf7aqdixjn9O0lDm/Hbg0ySXAicALknymqi5fxn6nZ6UX8Y/Fg7mt9Pp/JPzokJrVwPeYC+/DP7z88pC6/+LY/1F0SfMFLgK+Bcys9FxGzHPkd8bc2mn/j2XfnOT7PtaOJc45wI3Ax1Z6HkdrzgM1b+Q4+1F0xRs4Fg/m9k/9KvDd3p8v7J0/DfhSX90lzP3y/yBwzTzvdTwE+pLmCzzA3Hrk7t6xbaXntMBcj5gDsBXY2nsc4JO98buBziTf97F4LHbOwOuZW6rY0/fdXrLS81nu77nvPY67QPfWf0lqhP/KRZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRvwfLtly3MPxiRQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "#following generate sim_nos instance of simulation \n",
    "envs = gym.make('CartPole-v1')\n",
    "mean_rewards = []\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "    collect_trajectories(envs, actor, tmax=tmax)  \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    for _ in range(SGD_epoch):\n",
    "        L = -clipped_surrogate(actor, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "        print(L)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "\n",
    "    epsilon*=0.999\n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n",
    "plt.plot(mean_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_probs, states, actions, rewards = \\\n",
    "    collect_trajectories(envs, actor, tmax=tmax)  \n",
    "states = torch.from_numpy(np.array(states)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\akash\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[104.0438],\n",
       "        [136.3016],\n",
       "        [173.8584],\n",
       "        [210.0298],\n",
       "        [241.3389],\n",
       "        [266.8065],\n",
       "        [286.4225],\n",
       "        [300.9705],\n",
       "        [311.4409],\n",
       "        [318.7917]], device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = torch.tensor(actions, dtype=torch.int8, device=device).reshape(len(actions), 1)\n",
    "new_probs = actor(states).reshape(len(states),1)\n",
    "old_probs = torch.tensor(old_probs, dtype=torch.float, device=device).reshape(len(old_probs),1)\n",
    "new_probs = torch.where(actions == 1, new_probs, 1.0-new_probs)\n",
    "rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "new_probs/old_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount = discount**np.arange(len(rewards))\n",
    "rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.4487761] [2.59178994]\n",
      "[[ 1.690758  ]\n",
      " [ 1.30492426]\n",
      " [ 0.93645304]\n",
      " [ 0.58456302]\n",
      " [ 0.24850806]\n",
      " [-0.07242443]\n",
      " [-0.37891496]\n",
      " [-0.67161342]\n",
      " [-0.95114044]\n",
      " [-1.21808875]\n",
      " [-1.47302438]]\n"
     ]
    }
   ],
   "source": [
    "discount = 0.955**np.arange(len(rewards))\n",
    "rewards_te = np.multiply(rewards, discount).reshape(len(rewards),1)\n",
    "rewards_future = rewards_te[::-1].cumsum(axis=0)[::-1]\n",
    "mean = np.mean(rewards_future, axis = 0)\n",
    "std = np.std(rewards_future, axis = 0)\n",
    "print(mean, std)\n",
    "rewards_normalized = (rewards_future - mean)/std\n",
    "print(rewards_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.690758  ],\n",
       "       [ 1.30492426],\n",
       "       [ 0.93645304],\n",
       "       [ 0.58456302],\n",
       "       [ 0.24850806],\n",
       "       [-0.07242443],\n",
       "       [-0.37891496],\n",
       "       [-0.67161342],\n",
       "       [-0.95114044],\n",
       "       [-1.21808875],\n",
       "       [-1.47302438]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rewards_future - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 1.0 with action 0 with score tensor([-0.1227], device='cuda:0', grad_fn=<TanhBackward>) "
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        prob = actor.forward(state_tensor)\n",
    "        action_baseline = critic.forward(state_tensor)\n",
    "        action = 1 if prob.detach().cpu().numpy()>=0.5 else 0\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        print('\\rReward {} with action {} with score {}'.format(reward, action, action_baseline), end = ' ')\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
