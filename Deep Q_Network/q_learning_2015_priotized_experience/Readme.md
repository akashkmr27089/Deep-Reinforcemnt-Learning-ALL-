## The main reason why priortised sampling may work worst:

Algorithm uses priority replay. This algorithm gives replay memories with higher temporal difference errors a higher probability of being selected, because it means the RL was not able to predict the correct Q-values given those states, so by picking these states more often, your model will train to do better on these states. But the problem is that these states are only a subset of your whole state space, and so your model will be biased towards this subset, and perform poorly for the remainder of your state space. This is especially a problem as you train your model longer, because only a small set of your states will have very large errors. To avoid this, you can anneal out the priority replay. Please see the original paper here: https://arxiv.org/abs/1511.05952

You may also want to anneal out your learning rate, or increase the batch size as training goes on. These two are apparently equivalent according to a new paper published earlier this year at Google. https://openreview.net/forum?id=B1Yy1BxCZ This will allow your model to very slowly have a learning rate of 0 as training goes on and on, essentially stopping training after a while. Because if you never lower learning rate, an unlucky batch of bad data can potentially ruin the weights of your neural network.
