{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in c:\\users\\akash\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\akash\\appdata\\roaming\\python\\python36\\site-packages (from bayesian-optimization) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\akash\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from bayesian-optimization) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\akash\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from bayesian-optimization) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\akash\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "from bayes_opt  import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_wrapper(epsilon, alpha, gamma, eps_start):\n",
    "    agent = Agent(epsilon=epsilon, alpha=alpha, gamma=gamma, eps_start=eps_start)\n",
    "    avg_rewards, best_avg_reward = interact(env, agent, num_episodes)\n",
    "    return best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | eps_start |  epsilon  |   gamma   |\n",
      "-------------------------------------------------------------------------\n",
      "Episode 2000/2000 || Best average reward -251.75\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-251.8   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -303.86\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-303.9   \u001b[0m | \u001b[0m 0.1454  \u001b[0m | \u001b[0m 0.1952  \u001b[0m | \u001b[0m 0.07559 \u001b[0m | \u001b[0m 0.6757  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -281.1\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-281.1   \u001b[0m | \u001b[0m 0.383   \u001b[0m | \u001b[0m 0.1619  \u001b[0m | \u001b[0m 0.0681  \u001b[0m | \u001b[0m 0.7073  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -227.63\n",
      "\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-227.6   \u001b[0m | \u001b[95m 0.3824  \u001b[0m | \u001b[95m 0.05686 \u001b[0m | \u001b[95m 0.03304 \u001b[0m | \u001b[95m 0.512   \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -232.13\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-232.1   \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 0.06708 \u001b[0m | \u001b[0m 0.06768 \u001b[0m | \u001b[0m 0.6611  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -206.84\n",
      "\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-206.8   \u001b[0m | \u001b[95m 0.4805  \u001b[0m | \u001b[95m 0.01524 \u001b[0m | \u001b[95m 0.05155 \u001b[0m | \u001b[95m 0.9609  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -273.44\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-273.4   \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 0.1398  \u001b[0m | \u001b[0m 0.02369 \u001b[0m | \u001b[0m 0.9646  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -269.3\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-269.3   \u001b[0m | \u001b[0m 0.1347  \u001b[0m | \u001b[0m 0.1351  \u001b[0m | \u001b[0m 0.0984  \u001b[0m | \u001b[0m 0.9922  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -219.89\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-219.9   \u001b[0m | \u001b[0m 0.4337  \u001b[0m | \u001b[0m 0.04465 \u001b[0m | \u001b[0m 0.05629 \u001b[0m | \u001b[0m 0.6929  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -298.19\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-298.1   \u001b[0m | \u001b[0m 0.3833  \u001b[0m | \u001b[0m 0.1805  \u001b[0m | \u001b[0m 0.09953 \u001b[0m | \u001b[0m 0.8347  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -215.03\n",
      "\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-215.0   \u001b[0m | \u001b[0m 0.2344  \u001b[0m | \u001b[0m 0.03219 \u001b[0m | \u001b[0m 0.03523 \u001b[0m | \u001b[0m 0.7508  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -234.56\n",
      "\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-234.6   \u001b[0m | \u001b[0m 0.4685  \u001b[0m | \u001b[0m 0.06941 \u001b[0m | \u001b[0m 0.01853 \u001b[0m | \u001b[0m 0.5295  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -216.38\n",
      "\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-216.4   \u001b[0m | \u001b[0m 0.3064  \u001b[0m | \u001b[0m 0.0337  \u001b[0m | \u001b[0m 0.04042 \u001b[0m | \u001b[0m 0.727   \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -205.94\n",
      "\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m-205.9   \u001b[0m | \u001b[95m 0.4249  \u001b[0m | \u001b[95m 0.01327 \u001b[0m | \u001b[95m 0.0623  \u001b[0m | \u001b[95m 0.6802  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -262.1\n",
      "\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-262.1   \u001b[0m | \u001b[0m 0.2173  \u001b[0m | \u001b[0m 0.1199  \u001b[0m | \u001b[0m 0.04749 \u001b[0m | \u001b[0m 0.7587  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -221.15\n",
      "\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-221.2   \u001b[0m | \u001b[0m 0.1757  \u001b[0m | \u001b[0m 0.04389 \u001b[0m | \u001b[0m 0.04058 \u001b[0m | \u001b[0m 0.6196  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -203.87\n",
      "\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m-203.9   \u001b[0m | \u001b[95m 0.4126  \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 0.06012 \u001b[0m | \u001b[95m 0.6734  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -217.01\n",
      "\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-217.0   \u001b[0m | \u001b[0m 0.3914  \u001b[0m | \u001b[0m 0.0359  \u001b[0m | \u001b[0m 0.05909 \u001b[0m | \u001b[0m 0.544   \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -233.75\n",
      "\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-233.8   \u001b[0m | \u001b[0m 0.1389  \u001b[0m | \u001b[0m 0.06551 \u001b[0m | \u001b[0m 0.05081 \u001b[0m | \u001b[0m 0.7211  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -268.76\n",
      "\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-268.8   \u001b[0m | \u001b[0m 0.1731  \u001b[0m | \u001b[0m 0.1356  \u001b[0m | \u001b[0m 0.08887 \u001b[0m | \u001b[0m 0.9765  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -203.87\n",
      "\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-203.9   \u001b[0m | \u001b[0m 0.4087  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.06149 \u001b[0m | \u001b[0m 0.657   \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -203.87\n",
      "\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-203.9   \u001b[0m | \u001b[0m 0.4029  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.06222 \u001b[0m | \u001b[0m 0.6669  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -203.96\n",
      "\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-204.0   \u001b[0m | \u001b[0m 0.4081  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.05237 \u001b[0m | \u001b[0m 0.6652  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -208.37\n",
      "\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-208.4   \u001b[0m | \u001b[0m 0.4322  \u001b[0m | \u001b[0m 0.02151 \u001b[0m | \u001b[0m 0.06262 \u001b[0m | \u001b[0m 0.6768  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -264.26\n",
      "\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-264.3   \u001b[0m | \u001b[0m 0.3844  \u001b[0m | \u001b[0m 0.1263  \u001b[0m | \u001b[0m 0.05278 \u001b[0m | \u001b[0m 0.5438  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -203.87\n",
      "\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-203.9   \u001b[0m | \u001b[0m 0.4123  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.06161 \u001b[0m | \u001b[0m 0.6651  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -204.05\n",
      "\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-204.1   \u001b[0m | \u001b[0m 0.4068  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.05817 \u001b[0m | \u001b[0m 0.6718  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -204.41\n",
      "\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-204.4   \u001b[0m | \u001b[0m 0.396   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.06    \u001b[0m | \u001b[0m 0.6499  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -203.96\n",
      "\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-204.0   \u001b[0m | \u001b[0m 0.4036  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.06808 \u001b[0m | \u001b[0m 0.6646  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -204.14\n",
      "\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-204.1   \u001b[0m | \u001b[0m 0.4079  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.05491 \u001b[0m | \u001b[0m 0.6569  \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds = {'epsilon': (0.01, 0.1), 'alpha': (0.1, 0.5), 'gamma': (0.5, 1.0), 'eps_start': (0.01, 0.2)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=interact_wrapper,\n",
    "    pbounds=pbounds,\n",
    "    random_state=47\n",
    ")\n",
    "\n",
    "optimizer.probe(\n",
    "    params={'epsilon': 0.1, 'alpha': 0.1, 'gamma': 0.9, 'eps_start': 0.1},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=4,\n",
    "    n_iter=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
