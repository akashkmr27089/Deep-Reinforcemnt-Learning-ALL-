{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.14.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.20.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\akash\\appdata\\roaming\\python\\python36\\site-packages (from bayesian-optimization) (1.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 7 of c:\\programdata\\anaconda3\\lib\\site-packages\\pywin32.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"c:\\programdata\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named 'pywin32_bootstrap'\n",
      "\n",
      "Remainder of file ignored\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "from bayes_opt  import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_wrapper(epsilon, alpha, gamma, eps_start):\n",
    "    agent = Agent(epsilon=epsilon, alpha=alpha, gamma=gamma, eps_start=eps_start)\n",
    "    avg_rewards, best_avg_reward = interact(env, agent, num_episodes)\n",
    "    return best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | eps_start |  epsilon  |   gamma   |\n",
      "-------------------------------------------------------------------------\n",
      "Episode 10000/10000 || Best average reward -6.59\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-6.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -76.68\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-76.68   \u001b[0m | \u001b[0m 0.1454  \u001b[0m | \u001b[0m 0.1952  \u001b[0m | \u001b[0m 0.07559 \u001b[0m | \u001b[0m 0.6757  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -71.79\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-71.79   \u001b[0m | \u001b[0m 0.383   \u001b[0m | \u001b[0m 0.1619  \u001b[0m | \u001b[0m 0.0681  \u001b[0m | \u001b[0m 0.7073  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -65.03\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-65.03   \u001b[0m | \u001b[0m 0.3824  \u001b[0m | \u001b[0m 0.05686 \u001b[0m | \u001b[0m 0.03304 \u001b[0m | \u001b[0m 0.512   \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -16.3\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-16.3    \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 0.06708 \u001b[0m | \u001b[0m 0.06768 \u001b[0m | \u001b[0m 0.6611  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward 6.679\n",
      "\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 6.67    \u001b[0m | \u001b[95m 0.4805  \u001b[0m | \u001b[95m 0.01524 \u001b[0m | \u001b[95m 0.05155 \u001b[0m | \u001b[95m 0.9609  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -9.03\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-9.03    \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 0.1398  \u001b[0m | \u001b[0m 0.02369 \u001b[0m | \u001b[0m 0.9646  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -10.44\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-10.44   \u001b[0m | \u001b[0m 0.1347  \u001b[0m | \u001b[0m 0.1351  \u001b[0m | \u001b[0m 0.0984  \u001b[0m | \u001b[0m 0.9922  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -10.43\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-10.43   \u001b[0m | \u001b[0m 0.3543  \u001b[0m | \u001b[0m 0.1399  \u001b[0m | \u001b[0m 0.02427 \u001b[0m | \u001b[0m 0.9644  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -48.18\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-48.18   \u001b[0m | \u001b[0m 0.3833  \u001b[0m | \u001b[0m 0.1805  \u001b[0m | \u001b[0m 0.09953 \u001b[0m | \u001b[0m 0.8347  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -9.81\n",
      "\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-9.81    \u001b[0m | \u001b[0m 0.3587  \u001b[0m | \u001b[0m 0.1397  \u001b[0m | \u001b[0m 0.01184 \u001b[0m | \u001b[0m 0.9675  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -15.2\n",
      "\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-15.2    \u001b[0m | \u001b[0m 0.1407  \u001b[0m | \u001b[0m 0.06666 \u001b[0m | \u001b[0m 0.0673  \u001b[0m | \u001b[0m 0.6612  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward 0.266\n",
      "\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.26    \u001b[0m | \u001b[0m 0.3064  \u001b[0m | \u001b[0m 0.0337  \u001b[0m | \u001b[0m 0.04042 \u001b[0m | \u001b[0m 0.727   \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -12.42\n",
      "\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-12.42   \u001b[0m | \u001b[0m 0.4249  \u001b[0m | \u001b[0m 0.01327 \u001b[0m | \u001b[0m 0.0623  \u001b[0m | \u001b[0m 0.6802  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -21.23\n",
      "\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-21.23   \u001b[0m | \u001b[0m 0.2173  \u001b[0m | \u001b[0m 0.1199  \u001b[0m | \u001b[0m 0.04749 \u001b[0m | \u001b[0m 0.7587  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -13.55\n",
      "\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-13.55   \u001b[0m | \u001b[0m 0.1757  \u001b[0m | \u001b[0m 0.04389 \u001b[0m | \u001b[0m 0.04058 \u001b[0m | \u001b[0m 0.6196  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -18.03\n",
      "\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-18.03   \u001b[0m | \u001b[0m 0.419   \u001b[0m | \u001b[0m 0.01313 \u001b[0m | \u001b[0m 0.06011 \u001b[0m | \u001b[0m 0.6766  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -8.12\n",
      "\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-8.12    \u001b[0m | \u001b[0m 0.1875  \u001b[0m | \u001b[0m 0.04223 \u001b[0m | \u001b[0m 0.04234 \u001b[0m | \u001b[0m 0.6175  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -8.65\n",
      "\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-8.65    \u001b[0m | \u001b[0m 0.1389  \u001b[0m | \u001b[0m 0.06551 \u001b[0m | \u001b[0m 0.05081 \u001b[0m | \u001b[0m 0.7211  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -11.95\n",
      "\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-11.95   \u001b[0m | \u001b[0m 0.1731  \u001b[0m | \u001b[0m 0.1356  \u001b[0m | \u001b[0m 0.08887 \u001b[0m | \u001b[0m 0.9765  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -16.08\n",
      "\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-16.08   \u001b[0m | \u001b[0m 0.4222  \u001b[0m | \u001b[0m 0.01174 \u001b[0m | \u001b[0m 0.06551 \u001b[0m | \u001b[0m 0.6756  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -7.55\n",
      "\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-7.55    \u001b[0m | \u001b[0m 0.354   \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 0.02139 \u001b[0m | \u001b[0m 0.9653  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -11.02\n",
      "\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-11.02   \u001b[0m | \u001b[0m 0.1865  \u001b[0m | \u001b[0m 0.04237 \u001b[0m | \u001b[0m 0.04219 \u001b[0m | \u001b[0m 0.6177  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -12.98\n",
      "\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-12.98   \u001b[0m | \u001b[0m 0.1948  \u001b[0m | \u001b[0m 0.048   \u001b[0m | \u001b[0m 0.04122 \u001b[0m | \u001b[0m 0.6168  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -10.22\n",
      "\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-10.22   \u001b[0m | \u001b[0m 0.3566  \u001b[0m | \u001b[0m 0.1502  \u001b[0m | \u001b[0m 0.02422 \u001b[0m | \u001b[0m 0.9623  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward 6.368\n",
      "\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 6.3     \u001b[0m | \u001b[0m 0.4795  \u001b[0m | \u001b[0m 0.02446 \u001b[0m | \u001b[0m 0.05547 \u001b[0m | \u001b[0m 0.9606  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward 6.322\n",
      "\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 6.32    \u001b[0m | \u001b[0m 0.4835  \u001b[0m | \u001b[0m 0.02701 \u001b[0m | \u001b[0m 0.04684 \u001b[0m | \u001b[0m 0.959   \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -7.91\n",
      "\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-7.91    \u001b[0m | \u001b[0m 0.1394  \u001b[0m | \u001b[0m 0.06516 \u001b[0m | \u001b[0m 0.05288 \u001b[0m | \u001b[0m 0.7163  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward -10.96\n",
      "\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-10.96   \u001b[0m | \u001b[0m 0.154   \u001b[0m | \u001b[0m 0.05425 \u001b[0m | \u001b[0m 0.06682 \u001b[0m | \u001b[0m 0.6515  \u001b[0m |\n",
      "Episode 10000/10000 || Best average reward 6.567\n",
      "\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 6.56    \u001b[0m | \u001b[0m 0.4762  \u001b[0m | \u001b[0m 0.01268 \u001b[0m | \u001b[0m 0.04973 \u001b[0m | \u001b[0m 0.9662  \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds = {'epsilon': (0.01, 0.1), 'alpha': (0.1, 0.5), 'gamma': (0.5, 1.0), 'eps_start': (0.01, 0.2)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=interact_wrapper,\n",
    "    pbounds=pbounds,\n",
    "    random_state=47\n",
    ")\n",
    "\n",
    "optimizer.probe(\n",
    "    params={'epsilon': 0.1, 'alpha': 0.1, 'gamma': 0.9, 'eps_start': 0.1},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=4,\n",
    "    n_iter=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import IPython\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20000/20000 || Best average reward 7.738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(alpha = 0.4762 , eps_start = 0.01268, epsilon =  0.04973  , gamma = 0.9662)\n",
    "avg_rewards, best_avg_reward = interact(env, agent, num_episodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 1\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    reward_step = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(agent.Q[state])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward_step += reward\n",
    "        state = next_state\n",
    "        time.sleep(0.1)\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(state,action)\n",
    "        if done:\n",
    "            rewards.append(reward_step)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
