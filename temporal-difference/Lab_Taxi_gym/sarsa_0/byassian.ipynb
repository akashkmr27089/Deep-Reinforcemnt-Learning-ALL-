{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\akash\\appdata\\roaming\\python\\python36\\site-packages (from bayesian-optimization) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.14.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 7 of c:\\programdata\\anaconda3\\lib\\site-packages\\pywin32.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"c:\\programdata\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named 'pywin32_bootstrap'\n",
      "\n",
      "Remainder of file ignored\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "from bayes_opt  import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_wrapper(epsilon, alpha, gamma, eps_start):\n",
    "    agent = Agent(epsilon=epsilon, alpha=alpha, gamma=gamma, eps_start=eps_start)\n",
    "    avg_rewards, best_avg_reward = interact(env, agent, num_episodes)\n",
    "    return best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | eps_start |  epsilon  |   gamma   |\n",
      "-------------------------------------------------------------------------\n",
      "Episode 2000/2000 || Best average reward 1.2887\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 1.28    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -103.76\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-103.8   \u001b[0m | \u001b[0m 0.1454  \u001b[0m | \u001b[0m 0.1952  \u001b[0m | \u001b[0m 0.07559 \u001b[0m | \u001b[0m 0.6757  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -119.9\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-119.9   \u001b[0m | \u001b[0m 0.383   \u001b[0m | \u001b[0m 0.1619  \u001b[0m | \u001b[0m 0.0681  \u001b[0m | \u001b[0m 0.7073  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -68.047\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-68.04   \u001b[0m | \u001b[0m 0.3824  \u001b[0m | \u001b[0m 0.05686 \u001b[0m | \u001b[0m 0.03304 \u001b[0m | \u001b[0m 0.512   \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -18.83\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-18.83   \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 0.06708 \u001b[0m | \u001b[0m 0.06768 \u001b[0m | \u001b[0m 0.6611  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 5.4112\n",
      "\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 5.4     \u001b[0m | \u001b[95m 0.4805  \u001b[0m | \u001b[95m 0.01524 \u001b[0m | \u001b[95m 0.05155 \u001b[0m | \u001b[95m 0.9609  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -1.294\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.29    \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 0.1398  \u001b[0m | \u001b[0m 0.02369 \u001b[0m | \u001b[0m 0.9646  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 0.0795\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.07    \u001b[0m | \u001b[0m 0.1347  \u001b[0m | \u001b[0m 0.1351  \u001b[0m | \u001b[0m 0.0984  \u001b[0m | \u001b[0m 0.9922  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 0.5482\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.54    \u001b[0m | \u001b[0m 0.1344  \u001b[0m | \u001b[0m 0.1348  \u001b[0m | \u001b[0m 0.09834 \u001b[0m | \u001b[0m 0.9922  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -1.731\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-1.73    \u001b[0m | \u001b[0m 0.3427  \u001b[0m | \u001b[0m 0.1375  \u001b[0m | \u001b[0m 0.02702 \u001b[0m | \u001b[0m 0.9625  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 0.3332\n",
      "\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.33    \u001b[0m | \u001b[0m 0.3487  \u001b[0m | \u001b[0m 0.1387  \u001b[0m | \u001b[0m 0.02528 \u001b[0m | \u001b[0m 0.9636  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -2.571\n",
      "\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.57    \u001b[0m | \u001b[0m 0.3487  \u001b[0m | \u001b[0m 0.1387  \u001b[0m | \u001b[0m 0.0244  \u001b[0m | \u001b[0m 0.9644  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -15.36\n",
      "\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-15.36   \u001b[0m | \u001b[0m 0.3064  \u001b[0m | \u001b[0m 0.0337  \u001b[0m | \u001b[0m 0.04042 \u001b[0m | \u001b[0m 0.727   \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -48.46\n",
      "\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-48.46   \u001b[0m | \u001b[0m 0.4249  \u001b[0m | \u001b[0m 0.01327 \u001b[0m | \u001b[0m 0.0623  \u001b[0m | \u001b[0m 0.6802  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -21.66\n",
      "\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-21.66   \u001b[0m | \u001b[0m 0.2173  \u001b[0m | \u001b[0m 0.1199  \u001b[0m | \u001b[0m 0.04749 \u001b[0m | \u001b[0m 0.7587  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -24.25\n",
      "\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-24.25   \u001b[0m | \u001b[0m 0.1757  \u001b[0m | \u001b[0m 0.04389 \u001b[0m | \u001b[0m 0.04058 \u001b[0m | \u001b[0m 0.6196  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -0.967\n",
      "\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.96    \u001b[0m | \u001b[0m 0.3063  \u001b[0m | \u001b[0m 0.03412 \u001b[0m | \u001b[0m 0.0409  \u001b[0m | \u001b[0m 0.7282  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -1.286\n",
      "\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-1.28    \u001b[0m | \u001b[0m 0.3367  \u001b[0m | \u001b[0m 0.14    \u001b[0m | \u001b[0m 0.01956 \u001b[0m | \u001b[0m 0.9675  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -1.494\n",
      "\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-1.49    \u001b[0m | \u001b[0m 0.1389  \u001b[0m | \u001b[0m 0.06551 \u001b[0m | \u001b[0m 0.05081 \u001b[0m | \u001b[0m 0.7211  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -0.494\n",
      "\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.49    \u001b[0m | \u001b[0m 0.1731  \u001b[0m | \u001b[0m 0.1356  \u001b[0m | \u001b[0m 0.08887 \u001b[0m | \u001b[0m 0.9765  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 4.6141\n",
      "\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 4.61    \u001b[0m | \u001b[0m 0.4882  \u001b[0m | \u001b[0m 0.01755 \u001b[0m | \u001b[0m 0.0592  \u001b[0m | \u001b[0m 0.9626  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 0.9857\n",
      "\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.98    \u001b[0m | \u001b[0m 0.1676  \u001b[0m | \u001b[0m 0.1291  \u001b[0m | \u001b[0m 0.08916 \u001b[0m | \u001b[0m 0.9673  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 0.8938\n",
      "\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.89    \u001b[0m | \u001b[0m 0.1571  \u001b[0m | \u001b[0m 0.1305  \u001b[0m | \u001b[0m 0.08708 \u001b[0m | \u001b[0m 0.9634  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -0.244\n",
      "\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.24    \u001b[0m | \u001b[0m 0.1589  \u001b[0m | \u001b[0m 0.134   \u001b[0m | \u001b[0m 0.08758 \u001b[0m | \u001b[0m 0.9625  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 4.2238\n",
      "\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 4.22    \u001b[0m | \u001b[0m 0.4854  \u001b[0m | \u001b[0m 0.02047 \u001b[0m | \u001b[0m 0.06501 \u001b[0m | \u001b[0m 0.9602  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 1.4571\n",
      "\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 1.45    \u001b[0m | \u001b[0m 0.1706  \u001b[0m | \u001b[0m 0.1228  \u001b[0m | \u001b[0m 0.08993 \u001b[0m | \u001b[0m 0.9702  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 2.0695\n",
      "\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 2.06    \u001b[0m | \u001b[0m 0.1717  \u001b[0m | \u001b[0m 0.1277  \u001b[0m | \u001b[0m 0.09077 \u001b[0m | \u001b[0m 0.9671  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -2.645\n",
      "\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.64    \u001b[0m | \u001b[0m 0.1394  \u001b[0m | \u001b[0m 0.06516 \u001b[0m | \u001b[0m 0.05288 \u001b[0m | \u001b[0m 0.7163  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward -15.68\n",
      "\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-15.6    \u001b[0m | \u001b[0m 0.154   \u001b[0m | \u001b[0m 0.05425 \u001b[0m | \u001b[0m 0.06682 \u001b[0m | \u001b[0m 0.6515  \u001b[0m |\n",
      "Episode 2000/2000 || Best average reward 5.6314\n",
      "\n",
      "| \u001b[95m 30      \u001b[0m | \u001b[95m 5.6     \u001b[0m | \u001b[95m 0.4762  \u001b[0m | \u001b[95m 0.01268 \u001b[0m | \u001b[95m 0.04973 \u001b[0m | \u001b[95m 0.9662  \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds = {'epsilon': (0.01, 0.1), 'alpha': (0.1, 0.5), 'gamma': (0.5, 1.0), 'eps_start': (0.01, 0.2)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=interact_wrapper,\n",
    "    pbounds=pbounds,\n",
    "    random_state=47\n",
    ")\n",
    "\n",
    "optimizer.probe(\n",
    "    params={'epsilon': 0.1, 'alpha': 0.1, 'gamma': 0.9, 'eps_start': 0.1},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=4,\n",
    "    n_iter=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import IPython\n",
    "IPython.display.clear_output()\n",
    "num_episodes2 = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20000/20000 || Best average reward 8.094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(alpha = 0.4762 , eps_start = 0.01268, epsilon = 0.04973 , gamma = 0.9662)\n",
    "avg_rewards, best_avg_reward = interact(env, agent, num_episodes2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 5\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    reward_step = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(agent.Q[state])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward_step += reward\n",
    "        state = next_state\n",
    "        time.sleep(0.1)\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(state,action)\n",
    "        if done:\n",
    "            rewards.append(reward_step)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
