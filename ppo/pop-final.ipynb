{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "LEFT = 4\n",
    "RIGHT = 5\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* bkg_color variable is used to remove background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single(image, bkg_color = np.array([144, 72, 17])):\n",
    "    img = np.mean(image[34:-16:2,::2]-bkg_color, axis=-1)/255.\n",
    "    return img\n",
    "\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract bkg and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n",
    "                                    axis=-1)/255.\n",
    "    batch_input = np.swapaxes(list_of_images_prepro,0,1)\n",
    "    return torch.from_numpy(batch_input).float().to(device)\n",
    "    #return batch_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing if its working as planned \n",
    "* Croping the environment\n",
    "* Skipping 1 pixels in between \n",
    "* subtract background images\n",
    "* where we are skipping 1 pixed in resulting preprocessed image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this is for testing wheather it is working fine or not\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image \\n after background removal')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy \n",
    "\n",
    "## Making Policy Update using Reinforce Policy Gradient\n",
    "\n",
    "Here we define policy which takes input an image and output the probability of action. Here even though there are two actions ( Moving left and right), we only output single value $P_{\\rm right}$. For left action we can use $P_{\\rm left}$ = 1 - $P_{\\rm right}$\n",
    "\n",
    "### 1. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n",
    "\n",
    "# use your own policy!\n",
    "# policy=Policy().to(device)\n",
    "policy=Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Network Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (conv1): Conv2d(2, 4, kernel_size=(6, 6), stride=(2, 2), bias=False)\n",
       "  (conv2): Conv2d(4, 16, kernel_size=(6, 6), stride=(4, 4))\n",
       "  (fc1): Linear(in_features=1296, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing at arbitary random actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward value -1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "state = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(0)\n",
    "    state = next_state\n",
    "    print('\\rReward value {} '.format(reward), end=' ')\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Network without training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "state = env.reset()\n",
    "for _ in range(5):\n",
    "    frame1, reward1, done, _ = env.step(np.random.choice([4,5]))\n",
    "    frame2, reward2, done, _ = env.step(0)\n",
    "if not done:\n",
    "    for _ in range(2000):\n",
    "        env.render()\n",
    "        frame_input = preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "        action = 4 if random.random() < prob else 5\n",
    "        frame1, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Parallel Environment \n",
    "\n",
    "1. Instead of training throuh only one environement, we will use set of environment and use there action state value to train network\n",
    "2. All these are done parallelly so that the cpu takes part in simulation and data collatection which is then shifted to gpu for further computation \n",
    "3. In single instance, it collects n instance of data which is then used to train\n",
    "4. Doing Improvement in algorithm, involves normalising reward.\n",
    "5. For this, we creates multiple instance of simulation whose values are then integrated using Pipe. Each simulation is actually different different process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "class VecEnv(ABC):\n",
    "    \"\"\"\n",
    "    An abstract asynchronous, vectorized environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_envs, observation_space, action_space):\n",
    "        self.num_envs = num_envs\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all the environments and return an array of\n",
    "        observations, or a dict of observation arrays.\n",
    "        If step_async is still doing work, that work will\n",
    "        be cancelled and step_wait() should not be called\n",
    "        until step_async() is invoked again.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_async(self, actions):\n",
    "        \"\"\"\n",
    "        Tell all the environments to start taking a step\n",
    "        with the given actions.\n",
    "        Call step_wait() to get the results of the step.\n",
    "        You should not call this if a step_async run is\n",
    "        already pending.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_wait(self):\n",
    "        \"\"\"\n",
    "        Wait for the step taken with step_async().\n",
    "        Returns (obs, rews, dones, infos):\n",
    "         - obs: an array of observations, or a dict of\n",
    "                arrays of observations.\n",
    "         - rews: an array of rewards\n",
    "         - dones: an array of \"episode done\" booleans\n",
    "         - infos: a sequence of info objects\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up the environments' resources.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Step the environments synchronously.\n",
    "        This is available for backwards compatibility.\n",
    "        \"\"\"\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        #logger.warn('Render not defined for %s' % self)\n",
    "        pass\n",
    "        \n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        if isinstance(self, VecEnvWrapper):\n",
    "            return self.venv.unwrapped\n",
    "        else:\n",
    "            return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper.x\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.observation_space, env.action_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parallelEnv(VecEnv):\n",
    "    def __init__(self, env_name='PongDeterministic-v4',\n",
    "                 n=4, seed=None,\n",
    "                 spaces=None):\n",
    "\n",
    "        env_fns = [ gym.make(env_name) for _ in range(n) ]\n",
    "\n",
    "        if seed is not None:\n",
    "            for i,e in enumerate(env_fns):\n",
    "                e.seed(i+seed)\n",
    "        \n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        adopted from openai baseline\n",
    "        \"\"\"\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        observation_space, action_space = self.remotes[0].recv()\n",
    "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            for remote in self.remotes:            \n",
    "                remote.recv()\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "        self.closed = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Trajectories from all the parallel Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert states to probability, passing through the policy\n",
    "def states_to_prob(policy, states):\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1,*states.shape[-3:])\n",
    "    return policy(policy_input).view(states.shape[:-3])\n",
    "\n",
    "# return sum of log-prob divided by T\n",
    "# same thing as -policy_loss\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "    n=len(envs.ps)\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    envs.reset()\n",
    "    # start all parallel agents\n",
    "    envs.step([1]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        fr1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, _ = envs.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "        batch_input = preprocess_batch([fr1,fr2])\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0-probs)\n",
    "        # advance the game (0=no action)\n",
    "        # we take one action and skip game forward\n",
    "        fr1, re1, is_done, _ = envs.step(action)\n",
    "        fr2, re2, is_done, _ = envs.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, \\\n",
    "        action_list, reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Finally the training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 200\n",
    "SGD_epoch = 4\n",
    "episode = 100\n",
    "sim_nos = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8f61b0871d9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtimer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProgressBar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#following generate sim_nos instance of simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0menvs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparallelEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PongDeterministic-v4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msim_nos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmean_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-6910bc9c26b6>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env_name, n, seed, spaces)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;31m# if the main process crashes, we should not cause things to hang\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwork_remotes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mremote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "#following generate sim_nos instance of simulation \n",
    "envs = parallelEnv('PongDeterministic-v4', n=sim_nos, seed=1234)\n",
    "mean_rewards = []\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "    collect_trajectories(envs, policy, tmax=tmax)  \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    for _ in range(SDG_epoch):\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "\n",
    "    epsilon*=0.999\n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n",
    "plt.plot(mean_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model After Trainig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy, 'reinforce_final.policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model After Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "state = env.reset()\n",
    "for _ in range(5):\n",
    "    frame1, reward1, done, _ = env.step(np.random.choice([4,5]))\n",
    "    frame2, reward2, done, _ = env.step(0)\n",
    "if not done:\n",
    "    for _ in range(2000):\n",
    "        env.render()\n",
    "        frame_input = preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "        action = 4 if random.random() < prob else 5\n",
    "        frame1, _, done, _ = env.step(action)\n",
    "        frame2, _, done, _ = env.step(0)\n",
    "        time.sleep(0.1)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
