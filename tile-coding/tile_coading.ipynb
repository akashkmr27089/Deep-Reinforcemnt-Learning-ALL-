{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random \n",
    "import math\n",
    "from bayes_opt  import BayesianOptimization\n",
    "\n",
    "# Set plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(505);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-8.990e-01 -6.990e-01 -4.990e-01 -2.990e-01 -9.900e-02  1.010e-01  3.010e-01  5.010e-01  7.010e-01]\n",
      "  [-8.660e-01 -6.660e-01 -4.660e-01 -2.660e-01 -6.600e-02  1.340e-01  3.340e-01  5.340e-01  7.340e-01]\n",
      "  [-8.330e-01 -6.330e-01 -4.330e-01 -2.330e-01 -3.300e-02  1.670e-01  3.670e-01  5.670e-01  7.670e-01]\n",
      "  [-8.220e-01 -6.220e-01 -4.220e-01 -2.220e-01 -2.200e-02  1.780e-01  3.780e-01  5.780e-01  7.780e-01]\n",
      "  [-1.006e+01 -7.551e+00 -5.038e+00 -2.524e+00 -1.100e-02  2.502e+00  5.016e+00  7.529e+00  1.004e+01]\n",
      "  [-2.262e+01 -1.697e+01 -1.131e+01 -5.660e+00 -5.000e-03  5.650e+00  1.130e+01  1.696e+01  2.261e+01]]\n",
      "\n",
      " [[-8.000e-01 -6.000e-01 -4.000e-01 -2.000e-01  0.000e+00  2.000e-01  4.000e-01  6.000e-01  8.000e-01]\n",
      "  [-8.000e-01 -6.000e-01 -4.000e-01 -2.000e-01  0.000e+00  2.000e-01  4.000e-01  6.000e-01  8.000e-01]\n",
      "  [-8.000e-01 -6.000e-01 -4.000e-01 -2.000e-01  0.000e+00  2.000e-01  4.000e-01  6.000e-01  8.000e-01]\n",
      "  [-8.000e-01 -6.000e-01 -4.000e-01 -2.000e-01  0.000e+00  2.000e-01  4.000e-01  6.000e-01  8.000e-01]\n",
      "  [-1.005e+01 -7.540e+00 -5.027e+00 -2.513e+00  0.000e+00  2.513e+00  5.027e+00  7.540e+00  1.005e+01]\n",
      "  [-2.262e+01 -1.696e+01 -1.131e+01 -5.655e+00  0.000e+00  5.655e+00  1.131e+01  1.696e+01  2.262e+01]]\n",
      "\n",
      " [[-7.010e-01 -5.010e-01 -3.010e-01 -1.010e-01  9.900e-02  2.990e-01  4.990e-01  6.990e-01  8.990e-01]\n",
      "  [-7.340e-01 -5.340e-01 -3.340e-01 -1.340e-01  6.600e-02  2.660e-01  4.660e-01  6.660e-01  8.660e-01]\n",
      "  [-7.670e-01 -5.670e-01 -3.670e-01 -1.670e-01  3.300e-02  2.330e-01  4.330e-01  6.330e-01  8.330e-01]\n",
      "  [-7.780e-01 -5.780e-01 -3.780e-01 -1.780e-01  2.200e-02  2.220e-01  4.220e-01  6.220e-01  8.220e-01]\n",
      "  [-1.004e+01 -7.529e+00 -5.016e+00 -2.502e+00  1.100e-02  2.524e+00  5.038e+00  7.551e+00  1.006e+01]\n",
      "  [-2.261e+01 -1.696e+01 -1.130e+01 -5.650e+00  5.000e-03  5.660e+00  1.131e+01  1.697e+01  2.262e+01]]]\n"
     ]
    }
   ],
   "source": [
    "def create_tilings(lows, highs, tiling_specs):\n",
    "    # TODO: Implement this\n",
    "    final_gird = []\n",
    "    final_gird2 = []\n",
    "    for j in range(len(tiling_specs)):\n",
    "        final_gird = []\n",
    "        for i in range(len(lows)):\n",
    "            init = np.linspace(lows[i], highs[i], tiling_specs[j][0][0], endpoint=False)[1:] + tiling_specs[j][1][i]\n",
    "            final_gird.append(init)\n",
    "        final_gird2.append(final_gird)\n",
    "    #return np.array(final_gird2,dtype=np.float32)\n",
    "    return np.array(final_gird2)\n",
    "\n",
    "\n",
    "# Tiling specs: [(<bins>, <offsets>), ...]\n",
    "tiling_specs = [((10, 10, 10), (-0.099, -0.066, -0.033, -0.022, -0.011, -0.005)),\n",
    "                ((10, 10, 10), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)),\n",
    "                ((10, 10, 10), (0.099, 0.066, 0.033, 0.022, 0.011, 0.005))]\n",
    "tilings = create_tilings(env.observation_space.low, env.observation_space.high, tiling_specs)\n",
    "print(tilings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(sample, grid):\n",
    "    # TODO: Implement this\n",
    "    final_value = []\n",
    "    for i in range(len(grid)):\n",
    "        init = np.digitize(sample[i], grid[i], right = True)\n",
    "        final_value.append(int(init))\n",
    "    return final_value\n",
    "\n",
    "\n",
    "def tile_encode(sample, tilings, flatten=False):\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "    vector_bin = []\n",
    "    for i in range(len(tilings)):\n",
    "        value = discretize(sample, tilings[i])\n",
    "        vector_bin.append(value)\n",
    "    return vector_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_table = np.zeros(state_size + (action_size,))\n",
    "        print(\"QTable(): size =\", self.q_table.shape)\n",
    "\n",
    "class TiledQTable:\n",
    "    \"\"\"Composite Q-table with an internal tile coding scheme.\"\"\"\n",
    "    \n",
    "    def __init__(self, low, high, tiling_specs, action_size):\n",
    "        self.tilings = create_tilings(low, high, tiling_specs)\n",
    "        self.state_sizes = [tuple(len(splits)+1 for splits in tiling_grid) for tiling_grid in self.tilings]\n",
    "        self.action_size = action_size\n",
    "        self.q_tables = [QTable(state_size, self.action_size) for state_size in self.state_sizes]\n",
    "        print(\"TiledQTable(): no. of internal tables = \", len(self.q_tables))\n",
    "    \n",
    "    def get(self, state, action):\n",
    "        # TODO: Encode state to get tile indices\n",
    "        data = tile_encode(state, self.tilings)\n",
    "        #print(state, action, data)\n",
    "        val = 0\n",
    "        # TODO: Retrieve q-value for each tiling, and return their average\n",
    "        for i in range(len(data)):\n",
    "            val += self.q_tables[i].q_table[tuple(data[i]) + (action,)]\n",
    "        val /= len(data)\n",
    "        return val\n",
    "        \n",
    "        \n",
    "    def update(self, state, action, value, alpha=0.1):\n",
    "        # TODO: Encode state to get tile indices\n",
    "        data = tile_encode(state, self.tilings)\n",
    "        # TODO: Update q-value for each tiling by update factor alpha\n",
    "        for i in range(len(data)):\n",
    "            temp = self.q_tables[i].q_table[tuple(data[i]) + (action,)]\n",
    "            self.q_tables[i].q_table[tuple(data[i]) + (action,)] = alpha*value + (1.0 - alpha)*temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n"
     ]
    }
   ],
   "source": [
    "high = env.observation_space.high\n",
    "low = env.observation_space.low\n",
    "tq = TiledQTable(low, high, tiling_specs, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, epsilon, alpha, gamma, eps_start):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon;\n",
    "        self.eps_decay = 0.9999\n",
    "        self.eps_start = eps_start\n",
    "        self.high = env.observation_space.high\n",
    "        self.low = env.observation_space.low\n",
    "        self.nA = env.action_space.n;\n",
    "        self.tq = TiledQTable(low, high, tiling_specs, self.nA)\n",
    "        \n",
    "    def q_probs(self, state):\n",
    "        policy = np.ones((env.action_space.n))*(self.epsilon/self.nA)\n",
    "        posible_choice = []\n",
    "        for i in range(self.nA):\n",
    "            posible_choice.append(self.tq.get(state, i))\n",
    "        best_pos = np.argmax(posible_choice)\n",
    "        policy[best_pos] = (1 - self.epsilon) + self.epsilon/self.nA\n",
    "        return policy\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.nA), p=self.q_probs(state))\n",
    "        \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        old_val = tq.get(state, action)\n",
    "        if not done:\n",
    "            next_action = self.get_action(next_state)\n",
    "            #value = old_val + self.alpha*(reward + self.gamma*self.tq.get(next_state, next_action) - old_val)\n",
    "            value = reward + self.gamma*self.tq.get(next_state, action) - old_val\n",
    "        if done:\n",
    "            #value = old_val + self.alpha*(reward - old_val)\n",
    "            self.epsilon = max(self.epsilon*self.eps_decay, self.eps_start)\n",
    "            value = reward - old_val\n",
    "        #print(value)\n",
    "        self.tq.update(state, action, value, self.alpha)\n",
    "        #print(\"state {}, action {}, value {} :\".format(state, action, self.tq.get(state,action)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact(env, agent, num_episodes=1000, window=100):\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    best_avg_reward = -math.inf\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        samp_reward = 0\n",
    "        while True:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            samp_reward += reward\n",
    "            if done:\n",
    "                samp_rewards.append(samp_reward)\n",
    "                break\n",
    "        if (i_episode >= 100):\n",
    "            avg_reward = np.mean(samp_rewards)\n",
    "            avg_rewards.append(avg_reward)\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "        print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        if i_episode == num_episodes: print('\\n')\n",
    "    return avg_rewards, best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#avg_rewards, best_avg_reward = interact(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "def interact_wrapper(epsilon, alpha, gamma, eps_start):\n",
    "    agent = Agent(epsilon=epsilon, alpha=alpha, gamma=gamma, eps_start=eps_start)\n",
    "    avg_rewards, best_avg_reward = interact(env, agent, num_episodes)\n",
    "    return best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | eps_start |  epsilon  |   gamma   |\n",
      "-------------------------------------------------------------------------\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -381.82\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-381.8   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -329.86\n",
      "\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-329.9   \u001b[0m | \u001b[95m 0.1454  \u001b[0m | \u001b[95m 0.1952  \u001b[0m | \u001b[95m 0.07559 \u001b[0m | \u001b[95m 0.6757  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -334.33\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-334.3   \u001b[0m | \u001b[0m 0.383   \u001b[0m | \u001b[0m 0.1619  \u001b[0m | \u001b[0m 0.0681  \u001b[0m | \u001b[0m 0.7073  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -245.37\n",
      "\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-245.4   \u001b[0m | \u001b[95m 0.3824  \u001b[0m | \u001b[95m 0.05686 \u001b[0m | \u001b[95m 0.03304 \u001b[0m | \u001b[95m 0.512   \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -354.75\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-354.8   \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 0.06708 \u001b[0m | \u001b[0m 0.06768 \u001b[0m | \u001b[0m 0.6611  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -276.46\n",
      "\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-276.5   \u001b[0m | \u001b[0m 0.3825  \u001b[0m | \u001b[0m 0.05646 \u001b[0m | \u001b[0m 0.03293 \u001b[0m | \u001b[0m 0.5111  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -294.95\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-294.9   \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 0.1398  \u001b[0m | \u001b[0m 0.02369 \u001b[0m | \u001b[0m 0.9646  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -337.29\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-337.3   \u001b[0m | \u001b[0m 0.1347  \u001b[0m | \u001b[0m 0.1351  \u001b[0m | \u001b[0m 0.0984  \u001b[0m | \u001b[0m 0.9922  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -285.79\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-285.8   \u001b[0m | \u001b[0m 0.4337  \u001b[0m | \u001b[0m 0.04465 \u001b[0m | \u001b[0m 0.05629 \u001b[0m | \u001b[0m 0.6929  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -306.44\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-306.4   \u001b[0m | \u001b[0m 0.3833  \u001b[0m | \u001b[0m 0.1805  \u001b[0m | \u001b[0m 0.09953 \u001b[0m | \u001b[0m 0.8347  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -259.96\n",
      "\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-260.0   \u001b[0m | \u001b[0m 0.3814  \u001b[0m | \u001b[0m 0.06136 \u001b[0m | \u001b[0m 0.04184 \u001b[0m | \u001b[0m 0.5145  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -273.59\n",
      "\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-273.6   \u001b[0m | \u001b[0m 0.3783  \u001b[0m | \u001b[0m 0.05223 \u001b[0m | \u001b[0m 0.0331  \u001b[0m | \u001b[0m 0.5088  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -270.89\n",
      "\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-270.9   \u001b[0m | \u001b[0m 0.3064  \u001b[0m | \u001b[0m 0.0337  \u001b[0m | \u001b[0m 0.04042 \u001b[0m | \u001b[0m 0.727   \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -258.71\n",
      "\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-258.7   \u001b[0m | \u001b[0m 0.4249  \u001b[0m | \u001b[0m 0.01327 \u001b[0m | \u001b[0m 0.0623  \u001b[0m | \u001b[0m 0.6802  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -294.08\n",
      "\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-294.1   \u001b[0m | \u001b[0m 0.2173  \u001b[0m | \u001b[0m 0.1199  \u001b[0m | \u001b[0m 0.04749 \u001b[0m | \u001b[0m 0.7587  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -351.19\n",
      "\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-351.2   \u001b[0m | \u001b[0m 0.1757  \u001b[0m | \u001b[0m 0.04389 \u001b[0m | \u001b[0m 0.04058 \u001b[0m | \u001b[0m 0.6196  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -349.05\n",
      "\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-349.1   \u001b[0m | \u001b[0m 0.1794  \u001b[0m | \u001b[0m 0.1645  \u001b[0m | \u001b[0m 0.07105 \u001b[0m | \u001b[0m 0.6671  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -318.66\n",
      "\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-318.7   \u001b[0m | \u001b[0m 0.2067  \u001b[0m | \u001b[0m 0.119   \u001b[0m | \u001b[0m 0.0487  \u001b[0m | \u001b[0m 0.7537  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -330.89\n",
      "\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-330.9   \u001b[0m | \u001b[0m 0.1389  \u001b[0m | \u001b[0m 0.06551 \u001b[0m | \u001b[0m 0.05081 \u001b[0m | \u001b[0m 0.7211  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -350.94\n",
      "\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-350.9   \u001b[0m | \u001b[0m 0.1731  \u001b[0m | \u001b[0m 0.1356  \u001b[0m | \u001b[0m 0.08887 \u001b[0m | \u001b[0m 0.9765  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -236.71\n",
      "\n",
      "| \u001b[95m 21      \u001b[0m | \u001b[95m-236.7   \u001b[0m | \u001b[95m 0.4222  \u001b[0m | \u001b[95m 0.01174 \u001b[0m | \u001b[95m 0.06551 \u001b[0m | \u001b[95m 0.6756  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -290.82\n",
      "\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-290.8   \u001b[0m | \u001b[0m 0.34    \u001b[0m | \u001b[0m 0.1672  \u001b[0m | \u001b[0m 0.0152  \u001b[0m | \u001b[0m 0.9483  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -240.63\n",
      "\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-240.6   \u001b[0m | \u001b[0m 0.3123  \u001b[0m | \u001b[0m 0.03486 \u001b[0m | \u001b[0m 0.04112 \u001b[0m | \u001b[0m 0.7214  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -236.18\n",
      "\n",
      "| \u001b[95m 24      \u001b[0m | \u001b[95m-236.2   \u001b[0m | \u001b[95m 0.4322  \u001b[0m | \u001b[95m 0.02151 \u001b[0m | \u001b[95m 0.06262 \u001b[0m | \u001b[95m 0.6768  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -249.15\n",
      "\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-249.2   \u001b[0m | \u001b[0m 0.3844  \u001b[0m | \u001b[0m 0.1263  \u001b[0m | \u001b[0m 0.05278 \u001b[0m | \u001b[0m 0.5438  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -310.57\n",
      "\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-310.6   \u001b[0m | \u001b[0m 0.3789  \u001b[0m | \u001b[0m 0.05971 \u001b[0m | \u001b[0m 0.04352 \u001b[0m | \u001b[0m 0.5101  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -318.27\n",
      "\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-318.3   \u001b[0m | \u001b[0m 0.3893  \u001b[0m | \u001b[0m 0.1312  \u001b[0m | \u001b[0m 0.06336 \u001b[0m | \u001b[0m 0.54    \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -283.23\n",
      "\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-283.2   \u001b[0m | \u001b[0m 0.3076  \u001b[0m | \u001b[0m 0.03724 \u001b[0m | \u001b[0m 0.04774 \u001b[0m | \u001b[0m 0.7339  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 1000/1000 || Best average reward -296.23\n",
      "\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-296.2   \u001b[0m | \u001b[0m 0.154   \u001b[0m | \u001b[0m 0.05425 \u001b[0m | \u001b[0m 0.06682 \u001b[0m | \u001b[0m 0.6515  \u001b[0m |\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 27/1000 || Best average reward -inf"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.38297650845089537, 0.13478721950833067, 0.05859951898638705, 0.5458760812311984)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-d4d98cad2937>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer.maximize(\n\u001b[0;32m     15\u001b[0m     \u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-143-ecf9918de10f>\u001b[0m in \u001b[0;36minteract_wrapper\u001b[1;34m(epsilon, alpha, gamma, eps_start)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minteract_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meps_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mavg_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_avg_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minteract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest_avg_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-141-0086e43dfc44>\u001b[0m in \u001b[0;36minteract\u001b[1;34m(env, agent, num_episodes, window)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0msamp_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-140-ea3e675acd8f>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mold_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[1;31m#value = old_val + self.alpha*(reward + self.gamma*self.tq.get(next_state, next_action) - old_val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mold_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-140-ea3e675acd8f>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-140-ea3e675acd8f>\u001b[0m in \u001b[0;36mq_probs\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mposible_choice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mposible_choice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mbest_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposible_choice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_pos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-138-269c4c73d1ea>\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# TODO: Encode state to get tile indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtile_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtilings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m#print(state, action, data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-137-899b44b256ce>\u001b[0m in \u001b[0;36mtile_encode\u001b[1;34m(sample, tilings, flatten)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mvector_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtilings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtilings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mvector_bin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvector_bin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-137-899b44b256ce>\u001b[0m in \u001b[0;36mdiscretize\u001b[1;34m(sample, grid)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfinal_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mfinal_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbounds = {'epsilon': (0.01, 0.1), 'alpha': (0.1, 0.5), 'gamma': (0.5, 1.0), 'eps_start': (0.01, 0.2)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=interact_wrapper,\n",
    "    pbounds=pbounds,\n",
    "    random_state=47\n",
    ")\n",
    "\n",
    "optimizer.probe(\n",
    "    params={'epsilon': 0.1, 'alpha': 0.1, 'gamma': 0.9, 'eps_start': 0.1},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=4,\n",
    "    n_iter=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "Episode 10000/10000 || Best average reward -271.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(epsilon=0.06262, alpha=0.4322, gamma=0.6768, eps_start=0.02151)\n",
    "avg_rewards, best_avg_reward = interact(env, agent, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 8, 5, 0, 5, 4], [8, 8, 5, 0, 5, 4], [8, 7, 5, 0, 5, 4]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tilings = create_tilings(low, high, tiling_specs)\n",
    "entry = tile_encode([ 0.778, 0.628,  0.097,-0.995, 1.593, -1.5  ], tilings)\n",
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.09 , -3.092, -3.082])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.tq.q_tables[0].q_table[tuple(entry[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-500.0, -500.0, -500.0]\n"
     ]
    }
   ],
   "source": [
    "reward2 = []\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    rewards = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        posible_choice = []\n",
    "        for i in range(env.action_space.n):\n",
    "            posible_choice.append(agent.tq.get(state, i))\n",
    "        action = np.argmax(posible_choice)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            reward2.append(rewards)\n",
    "            break\n",
    "env.close()\n",
    "print(reward2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
